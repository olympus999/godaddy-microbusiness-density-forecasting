{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8988304-69fc-4e3d-a15e-b7da71b87192",
   "metadata": {},
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada2126-7935-4780-b60b-62d39225d607",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import/read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b590acc4-31dc-4c94-8155-19e68c214486",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "from typing import Callable\n",
    "\n",
    "import bayes_opt as bayes\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "from meteostat import Monthly, Point, Stations\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from traitlets import (\n",
    "    Any,\n",
    "    Bool,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Float,\n",
    "    HasTraits,\n",
    "    Int,\n",
    "    List,\n",
    "    TraitError,\n",
    "    TraitType,\n",
    "    Tuple,\n",
    "    Unicode,\n",
    "    default,\n",
    "    validate,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa7b749-b179-491b-84de-b586c94dbd71",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "boundaries_sub_data_path = \"other/boundaries\"\n",
    "bayesian_run_path = \"../data/bayesian_runs/\"\n",
    "\n",
    "\n",
    "def r(filename, sub_folder=\"kaggle\", delimiter=\",\"):\n",
    "    return pd.read_csv(\n",
    "        os.path.join(data_path, sub_folder, filename), delimiter=delimiter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a255db8-0b36-465b-a5a0-90471a4beb29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_census = r(\"census_starter.csv\")\n",
    "df_test = r(\"test.csv\")\n",
    "df_train = r(\"train.csv\")\n",
    "df_submission = r(\"sample_submission.csv\")\n",
    "\n",
    "df_boundaries = r(\"us-county-boundaries.csv\", boundaries_sub_data_path, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072cd32-0f14-473b-b161-55800bde1b6f",
   "metadata": {},
   "source": [
    "### Add census, year, fix dates etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45961f8-b080-400f-b795-49075b1a9c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fix_df_train(df_train):\n",
    "    df = df_train.copy()\n",
    "\n",
    "    # Add year\n",
    "    df[\"first_day_of_month\"] = pd.to_datetime(df[\"first_day_of_month\"])\n",
    "    df[\"year\"] = df[\"first_day_of_month\"].dt.year.astype(int)\n",
    "\n",
    "    # Add df_census to df\n",
    "    cols = list(df_census.columns)\n",
    "    cols.remove(\"cfips\")\n",
    "\n",
    "    t0 = df_census.melt(\"cfips\", cols)\n",
    "    t0[\"year\"] = t0[\"variable\"].str.split(\"_\").str[-1].astype(int)\n",
    "    t0[\"variable_name\"] = t0[\"variable\"].str.rsplit(\"_\", expand=False, n=1).str[0]\n",
    "\n",
    "    t1 = pd.pivot_table(t0, \"value\", [\"cfips\", \"year\"], \"variable_name\").reset_index()\n",
    "\n",
    "    # Census data is lagging 2 years\n",
    "    t1[\"year\"] = t1[\"year\"] + 2\n",
    "\n",
    "    df = pd.merge(df, t1, \"left\", left_on=[\"cfips\", \"year\"], right_on=[\"cfips\", \"year\"])\n",
    "\n",
    "    # Add month\n",
    "    df[\"month\"] = df[\"first_day_of_month\"].dt.month\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6aafbe-6bd4-4ab9-8c8e-e04af6fc517a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = fix_df_train(df_train)\n",
    "\n",
    "t = df_train[df_train.isna().any(axis=1)]\n",
    "if t.shape[0] != 22:\n",
    "    raise Exception(\"Nan counts used to be 22... something changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26862e-21d2-48ef-9acb-48e8a6fdb8d2",
   "metadata": {},
   "source": [
    "### Weather save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30b5b17-6d62-42d9-ab11-fb4cf193fcf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_weather_data(path_weather):\n",
    "    # Temperatures\n",
    "    temps = df_boundaries[[\"NAME\", \"NAMELSAD\", \"INTPTLAT\", \"INTPTLON\"]].copy()\n",
    "    temps[\"min_date\"] = df_train[\"first_day_of_month\"].min()\n",
    "    temps[\"max_date\"] = df_train[\"first_day_of_month\"].max()\n",
    "\n",
    "    data_list = []\n",
    "    for idx, row in temps.iterrows():\n",
    "        p = Point(row[\"INTPTLAT\"], row[\"INTPTLON\"], 70)\n",
    "\n",
    "        data = Monthly(p, row[\"min_date\"], row[\"max_date\"])\n",
    "        data = data.fetch()\n",
    "\n",
    "        if data.shape[0] > 0:\n",
    "            data[\"state\"] = row[\"NAME\"]\n",
    "            data[\"county\"] = row[\"NAMELSAD\"]\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "\n",
    "    weather_data = pd.concat(data_list)\n",
    "    weather_data.to_csv(path_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3185d92-86b2-43c8-807f-4fc0cd9bd87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_weather = \"../data/other/weather/weather.csv\"\n",
    "# save_weather_data(path_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134983df-a2b2-461b-bbb5-2809a2d7b622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv(path_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61789dd-7f47-4fdb-a1be-fdc2b65dd119",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9438a-2bd8-478f-a808-05c406971463",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a0b87e9-0f6e-4c98-9b63-9d4a335b6b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_feature_targets_history(t, feature_type, cols_f: dict, **kwargs):\n",
    "    \"\"\"\n",
    "    Add rolling windows and/or shifted values\n",
    "\n",
    "    window: list\n",
    "        List of windows to add. [2, 5] will add two columns with rolling window 2 and 5.\n",
    "    shifts: list\n",
    "        List of recents shifts to add. [2, 5] will add two columns with shifts of 2 and 5.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_rolling_mean_x(\n",
    "        t: pd.DataFrame, added_feature_cols: list, f_col: str, window: int\n",
    "    ):\n",
    "        rolling_mean = (\n",
    "            t.sort_values([\"cfips\", \"first_day_of_month\"])\n",
    "            .groupby([\"cfips\"])[\"microbusiness_density\"]\n",
    "            .rolling(window)\n",
    "            .mean()\n",
    "            .rename(f_col)\n",
    "        )\n",
    "\n",
    "        return rolling_mean\n",
    "\n",
    "    def add_shifted_x(\n",
    "        t: pd.DataFrame, f_col: str, shift: int\n",
    "    ):\n",
    "        previous = (\n",
    "            t.sort_values([\"cfips\", \"first_day_of_month\"])\n",
    "            .groupby([\"cfips\"])[\"microbusiness_density\"]\n",
    "            .shift(shift)\n",
    "            .rename(f_col)\n",
    "        )\n",
    "\n",
    "        return t\n",
    "\n",
    "    def _loop_new_cols(t, f, new_cols):\n",
    "        res: list[pd.DataFrame] = []\n",
    "        for col, val in cols_f.items():\n",
    "            r =  f(t, col, v)\n",
    "        \n",
    "    \n",
    "\n",
    "    if feature_type == 'target_rolling_mean':\n",
    "        # Rolling mean target\n",
    "        t = _loop_dict(t, add_rolling_mean_x)\n",
    "    elif feature_type == 'target_shift':\n",
    "        # Previous target values\n",
    "        t = _loop_dict(t, add_shifted_x)\n",
    "    else:\n",
    "        raise ValueError('Kind ´{kind}´ is not supported')\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a23d146a-1958-4b1f-8f36-577e1dc17e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_feature_targets_groupby_stats(\n",
    "    df: pd.DataFrame,\n",
    "    added_feature_cols: list,\n",
    "    cols_groupby_target: list = [\"cfips\", \"county\", \"state\"],\n",
    "    **kwargs\n",
    "):\n",
    "    def make_feature(\n",
    "        df: pd.DataFrame,\n",
    "        col: str,\n",
    "        col_template: str,\n",
    "        added_feature_cols: list,\n",
    "        agg_functions: list = [\"mean\", \"std\", \"median\"],\n",
    "    ):\n",
    "        \n",
    "        t0 = df.groupby(col)[\"microbusiness_density\"].agg(agg_functions)\n",
    "        new_cols = [col_template.format(col, x) for x in t0.columns]\n",
    "        t0.columns = new_cols\n",
    "        t0 = t0.reset_index()\n",
    "        t0[col] = t0[col].astype(df[col].dtype)\n",
    "\n",
    "        df = pd.merge(df, t0, \"left\", left_on=col, right_on=col)\n",
    "        \n",
    "        for f_col in list(t0.columns):\n",
    "            print(f_col)\n",
    "            added_feature_cols.append(f_col)\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print('t0.columns', t0.columns)\n",
    "        print('df.columns', df.columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "    col_template = \"target_{}_{}\"\n",
    "    \n",
    "    for col in cols_groupby_target:\n",
    "        df = make_feature(df, col, col_template, added_feature_cols)\n",
    "\n",
    "    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<')\n",
    "    print('return add_feature_targets_groupby_stats df.columns', df.columns)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf25a1f0-c86c-4ceb-8069-d56f4274548d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001     39\n",
       "39133    39\n",
       "39089    39\n",
       "39091    39\n",
       "39093    39\n",
       "         ..\n",
       "21113    39\n",
       "21115    39\n",
       "21117    39\n",
       "21119    39\n",
       "56045    39\n",
       "Name: cfips, Length: 3135, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"cfips\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0251191f-e83b-4ce5-b711-83445bfb3322",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correlated_states(df_train):\n",
    "    \"\"\"\n",
    "    Find states that correlate well in terms of change in ´microbusiness_density´\n",
    "    \"\"\"\n",
    "    cols_state_relation = []\n",
    "    dfs = pd.DataFrame([])\n",
    "    new_col_raw = \"mean\"\n",
    "\n",
    "    # Rolling\n",
    "    t0 = (\n",
    "        df_train.groupby([\"state\", \"first_day_of_month\"])[\"microbusiness_density\"]\n",
    "        .mean()\n",
    "        .rename(new_col_raw)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    t1 = t0.pivot_table(new_col_raw, \"first_day_of_month\", \"state\").sort_index().corr()\n",
    "    # plt.imshow(t1.values, cmap=\"hot\", interpolation=\"nearest\")\n",
    "    # plt.show()\n",
    "\n",
    "    t5 = t1.rename_axis([\"other_state\"], axis=1).stack().rename(\"corr\").reset_index()\n",
    "    t5 = t5[t5[\"state\"] != t5[\"other_state\"]]\n",
    "\n",
    "    # Clean pairs of same correlations\n",
    "    t5 = t5.sort_values(\"corr\").reset_index(drop=True)\n",
    "    cols = [\"state\", \"other_state\"]\n",
    "    t5[cols] = pd.DataFrame(np.sort(t5[cols].values, axis=1), columns=cols)\n",
    "    t5 = t5.drop_duplicates()\n",
    "\n",
    "    # Cluster\n",
    "    clustering = DBSCAN(eps=0.01, min_samples=2).fit(t5[\"corr\"].values.reshape(-1, 1))\n",
    "    t5[\"cluster\"] = clustering.labels_\n",
    "\n",
    "    # Iterate through pairs and add state means to each other\n",
    "    corr_states = t5[abs(t5[\"corr\"]) > 0.9]\n",
    "\n",
    "    # Append one month.\n",
    "    t2 = df_train.copy()\n",
    "    t2[\"first_day_of_month\"] = t2[\"first_day_of_month\"] + pd.DateOffset(months=1)\n",
    "    t0 = (\n",
    "        t2.groupby([\"state\", \"first_day_of_month\"])[\"microbusiness_density\"]\n",
    "        .mean()\n",
    "        .rename(new_col_raw)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Iterate over the pairs\n",
    "    for pair in corr_states.values:\n",
    "        group = pair[:2]\n",
    "        corr = pair[2]\n",
    "\n",
    "        switched = t0[t0[\"state\"].isin(group)].copy()\n",
    "\n",
    "        state_0 = group[0]\n",
    "        state_1 = group[1]\n",
    "\n",
    "        s = \"{}_{}_rolling_microbusiness_density\"\n",
    "        col_state_0 = s.format(state_0, state_1)\n",
    "        col_state_1 = s.format(state_1, state_0)\n",
    "\n",
    "        sw = pd.pivot_table(\n",
    "            switched, new_col_raw, \"first_day_of_month\", \"state\"\n",
    "        ).rename(columns={state_0: col_state_0, state_1: col_state_1})\n",
    "\n",
    "        def boo(df, sw, state, col_state, corr):\n",
    "            \"\"\"\n",
    "            I think it switches state labels.\n",
    "            \"\"\"\n",
    "            corr_col = \"corr_\" + col_state\n",
    "            # sw[corr_col] = corr\n",
    "\n",
    "            df_state_t = sw[[col_state]].reset_index()\n",
    "            df_state_t[\"state\"] = state\n",
    "\n",
    "            # df = pd.merge(\n",
    "            #     df,\n",
    "            #     df_state_t,\n",
    "            #     \"left\",\n",
    "            # left_on=[\"state\", \"first_day_of_month\"],\n",
    "            # right_on=[\"state\", \"first_day_of_month\"],\n",
    "            # )\n",
    "\n",
    "            return (df_state_t, corr_col)\n",
    "\n",
    "        df_state_t_0, corr_col_0 = boo(df_train, sw, state_1, col_state_0, corr)\n",
    "        df_state_t_1, corr_col_1 = boo(df_train, sw, state_0, col_state_1, corr)\n",
    "\n",
    "        # Append results to list\n",
    "        if dfs.shape[0] == 0:\n",
    "            dfs = pd.merge(\n",
    "                df_state_t_0,\n",
    "                df_state_t_1,\n",
    "                \"outer\",\n",
    "                left_on=[\"state\", \"first_day_of_month\"],\n",
    "                right_on=[\"state\", \"first_day_of_month\"],\n",
    "            )\n",
    "        else:\n",
    "            dfs = pd.merge(\n",
    "                dfs,\n",
    "                df_state_t_0,\n",
    "                \"outer\",\n",
    "                left_on=[\"state\", \"first_day_of_month\"],\n",
    "                right_on=[\"state\", \"first_day_of_month\"],\n",
    "            )\n",
    "            dfs = pd.merge(\n",
    "                dfs,\n",
    "                df_state_t_1,\n",
    "                \"outer\",\n",
    "                left_on=[\"state\", \"first_day_of_month\"],\n",
    "                right_on=[\"state\", \"first_day_of_month\"],\n",
    "            )\n",
    "        # cols_state_relation.extend([corr_col_0, corr_col_1])\n",
    "        cols_state_relation.extend([col_state_0, col_state_1])\n",
    "\n",
    "    return (dfs, cols_state_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b4e25b0-b974-4e03-9d8e-1323c9ca508b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def state_cluster(df_train):\n",
    "    t0 = df_train.groupby(\"state\")[\"microbusiness_density\"].agg([\"mean\", \"std\"])\n",
    "    clustering = DBSCAN(eps=0.5, min_samples=2).fit(t0.values)\n",
    "    t0[\"cluster\"] = clustering.labels_\n",
    "\n",
    "    return t0.reset_index()[[\"state\", \"cluster\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a409957-fbb8-40db-9b66-e112ff00cecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_arrow(df: pd.DataFrame, added_feature_cols):\n",
    "    def NormalizeData(data):\n",
    "        return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "    f_col = \"time_arrow\"\n",
    "\n",
    "    seconds_since = df[\"first_day_of_month\"].astype(\"int64\") // 1e9\n",
    "    df[f_col] = NormalizeData(seconds_since)\n",
    "\n",
    "    added_feature_cols.append(f_col)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8881615-5e7e-4ed6-b4e3-419911cdff76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            state  cluster\n",
       "0         Alabama        0\n",
       "1          Alaska        1\n",
       "2         Arizona        2\n",
       "3        Arkansas        0\n",
       "4      California        3\n",
       "..            ...      ...\n",
       "46       Virginia        1\n",
       "47     Washington        2\n",
       "48  West Virginia        0\n",
       "49      Wisconsin        0\n",
       "50        Wyoming       -1\n",
       "\n",
       "[51 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_cluster(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda891e1-ae14-45f5-bbb7-3d4f1bc091cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Maybe pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27f5b64-eac0-4277-b158-941adedd8ce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ups_downs(df_train):\n",
    "    \"\"\"\n",
    "    Counts ups and downs until the latest known time. Returns the percetage of ups/downs\n",
    "    \"\"\"\n",
    "    col = \"microbusiness_shift_bool_over_pct\"\n",
    "    col_to_group = \"cfips\"\n",
    "\n",
    "    t = df_train.copy()\n",
    "    t[\"microbusiness_shift_diff\"] = (\n",
    "        t[\"microbusiness_density\"]\n",
    "        - df_train.sort_values([col_to_group, \"first_day_of_month\"])\n",
    "        .groupby(col_to_group)\n",
    "        .shift()[\"microbusiness_density\"]\n",
    "    )\n",
    "\n",
    "    idx_over_0 = t[t[\"microbusiness_shift_diff\"] >= 0].index\n",
    "    idx_under_0 = t[t[\"microbusiness_shift_diff\"] < 0].index\n",
    "\n",
    "    t.loc[idx_over_0, \"microbusiness_shift_bool_over\"] = True\n",
    "    t.loc[idx_under_0, \"microbusiness_shift_bool_over\"] = False\n",
    "\n",
    "    t[\"microbusiness_shift_bool_over_sum\"] = (\n",
    "        t.groupby(col_to_group)[\"microbusiness_shift_bool_over\"]\n",
    "        .expanding()\n",
    "        .sum()\n",
    "        .values\n",
    "    )\n",
    "    t[\"microbusiness_shift_bool_over_count\"] = (\n",
    "        t.groupby(col_to_group)[\"microbusiness_shift_bool_over\"]\n",
    "        .expanding()\n",
    "        .count()\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    t[\"microbusiness_shift_bool_over_pct\"] = (\n",
    "        t[\"microbusiness_shift_bool_over_sum\"]\n",
    "        / t[\"microbusiness_shift_bool_over_count\"]\n",
    "    )\n",
    "\n",
    "    idx = t[t[\"microbusiness_shift_bool_over_count\"] < 3].index\n",
    "    t.loc[idx, \"microbusiness_shift_bool_over_pct\"] = np.nan\n",
    "\n",
    "    added_feature_cols.append(col)\n",
    "\n",
    "    return t[[\"row_id\", \"microbusiness_shift_bool_over_pct\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa1147-aa4f-4231-901c-ff64570301e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Misc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43575304-d47c-4b00-a7f1-128c2d31599b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dates(df_train):\n",
    "    \"\"\"\n",
    "    Split dates. Used for splitting train/test according to datetime.\n",
    "    \"\"\"\n",
    "    dates = np.sort(df_train[\"first_day_of_month\"].unique())\n",
    "    c = int(dates.shape[0] * 0.70)\n",
    "    dates_train = dates[:c]\n",
    "    dates_val = dates[c:]\n",
    "\n",
    "    return (dates_train, dates_val)\n",
    "\n",
    "\n",
    "def remove_outliers(train, outlier_multiplier):\n",
    "    \"\"\"\n",
    "    Remove outlies. Should be run on train only\n",
    "    \"\"\"\n",
    "    max_density = (\n",
    "        train.groupby(\"cfips\")[\"microbusiness_density\"]\n",
    "        .mean()\n",
    "        .rename(\"max_microbusiness_density\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    max_density[\"max_microbusiness_density\"] = (\n",
    "        max_density[\"max_microbusiness_density\"] * outlier_multiplier\n",
    "    )\n",
    "    t1 = pd.merge(\n",
    "        train.reset_index(), max_density, \"left\", left_on=\"cfips\", right_on=\"cfips\"\n",
    "    )\n",
    "\n",
    "    idx = t1[t1[\"microbusiness_density\"] < t1[\"max_microbusiness_density\"]][\"index\"]\n",
    "    t = train.loc[idx].copy()\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def smape(pred, eval_data):\n",
    "    if hasattr(eval_data, \"label\"):\n",
    "        A = eval_data.label  # Used by lightgbm\n",
    "    else:\n",
    "        A = eval_data  # Used by numpy\n",
    "    F = pred\n",
    "\n",
    "    if type(pred) == int or type(pred) == float:\n",
    "        # Single cases\n",
    "        value = 100 / 1 * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "    else:\n",
    "        # Many cases\n",
    "        value = 100 / len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "    return \"smape\", value, False\n",
    "\n",
    "\n",
    "def remove_empty_folders(path_abs):\n",
    "    walk = list(os.walk(path_abs))\n",
    "    for path, _, _ in walk[::-1]:\n",
    "        if len(os.listdir(path)) == 0:\n",
    "            os.rmdir(path)\n",
    "\n",
    "\n",
    "def save_bayesian_results(loss_fn: str, results: list[dict], best: dict):\n",
    "    def save_pkl(full_path, data):\n",
    "        with open(full_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    # Create folders\n",
    "    dt = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    subfolder = \"{}_{}\".format(dt, abs(round(best[\"target\"], 4)))\n",
    "    bay_result_dir = os.path.join(bayesian_run_path, loss_fn, subfolder)\n",
    "\n",
    "    if not os.path.exists(bay_result_dir):\n",
    "        os.makedirs(bay_result_dir)\n",
    "\n",
    "    # Full paths\n",
    "    result_path = os.path.join(bay_result_dir, \"result.pkl\")\n",
    "    best_path = os.path.join(bay_result_dir, \"best.pkl\")\n",
    "\n",
    "    # Save pickles\n",
    "    save_pkl(result_path, results)\n",
    "    save_pkl(best_path, best)\n",
    "\n",
    "    # Remove empty folders that can trash the place\n",
    "    remove_empty_folders(bayesian_run_path)\n",
    "\n",
    "\n",
    "def read_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9b584-ce97-4013-ba64-1131c47bfa16",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c698ef7-b7b6-46e6-b679-5f4ff0db680f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = \"\\033[95m\"\n",
    "    OKBLUE = \"\\033[94m\"\n",
    "    OKCYAN = \"\\033[96m\"\n",
    "    OKGREEN = \"\\033[92m\"\n",
    "    WARNING = \"\\033[93m\"\n",
    "    FAIL = \"\\033[91m\"\n",
    "    ENDC = \"\\033[0m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNDERLINE = \"\\033[4m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4c378-b3a5-45d4-b7a8-6666f1f34c3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, df_train):\n",
    "        self.df_train = df_train.copy()  # Original data\n",
    "\n",
    "        self._col_target: list = [\"microbusiness_density\"]\n",
    "        self._col_target_shifted: str = [\"shifted_microbusiness_density\"]\n",
    "        self._col_cfips: str = \"cfips\"\n",
    "        self._col_first_day_of_month = \"first_day_of_month\"\n",
    "\n",
    "        self.dates_train: np.array = None\n",
    "        self.dates_val: np.array = None\n",
    "\n",
    "        self._lgb_train: lgb.Dataset = None\n",
    "        self._lgb_eval: lgb.Dataset = None\n",
    "        # self._df: pd.DataFrame = None\n",
    "        self._cat_f: list = None\n",
    "        self._features: list = None\n",
    "\n",
    "        # self._debug_different_shifts = {} # Debugging purposes\n",
    "\n",
    "    def prepare_data_for_model(self, args):\n",
    "        # Split and validation strategy\n",
    "        for shift in args[\"shifts\"]:\n",
    "            # print('shift ---', shift)\n",
    "\n",
    "            added_feature_cols = []\n",
    "            cols_drop_or_not = [\n",
    "                # Existing features\n",
    "                \"state\",\n",
    "                \"pct_college\",\n",
    "                \"shift_in_months\",\n",
    "                \"pct_it_workers\",\n",
    "                \"pct_bb\",\n",
    "                \"pct_foreign_born\",\n",
    "                \"median_hh_inc\",\n",
    "                # \"month\",\n",
    "                \"cfips\",\n",
    "                \"county\",\n",
    "                # New features\n",
    "                \"time_arrow\",\n",
    "                ## Stats\n",
    "                \"target_cfips_std\",\n",
    "                \"target_cfips_median\",\n",
    "                \"target_county_median\",\n",
    "                \"target_county_mean\",\n",
    "                \"target_state_mean\",\n",
    "                \"target_cfips_mean\",\n",
    "                \"target_county_std\",\n",
    "                \"target_state_median\",\n",
    "                \"target_state_std\",\n",
    "                ## Roll\n",
    "                \"target_mean_rolling_1_activated\",\n",
    "                \"target_mean_rolling_2_activated\",\n",
    "                \"target_mean_rolling_3_activated\",\n",
    "                \"target_mean_rolling_4_activated\",\n",
    "                ## Shift \n",
    "                \"target_shift_1_activated\",\n",
    "                \"target_shift_2_activated\",\n",
    "                \"target_shift_3_activated\",\n",
    "                \"target_shift_4_activated\",\n",
    "            ]\n",
    "\n",
    "            # Do not modify df_train\n",
    "            df = self.df_train.copy()\n",
    "\n",
    "            # Define Train/Eval split\n",
    "            self.dates_train, self.dates_val = split_dates(df)\n",
    "\n",
    "            # Shift target features so we can predict 1, 2, 3... months ahead\n",
    "            df = self._shift_target(df, shift, added_feature_cols)\n",
    "\n",
    "            # Define columns to be used, feature engineering etc\n",
    "            df, self._cat_f = self._create_data(\n",
    "                df, added_feature_cols, args\n",
    "            )\n",
    "            print('-------------->>>>>>>>>>>>>><<<<<<<<<<<<<-----------')\n",
    "            print('After FEATURE ENG df.columns', df.columns)\n",
    "\n",
    "            # Remove some columns according to bayesian.\n",
    "            # df = self._drop_col_maybe(df, cols_drop_or_not, args)\n",
    "            self._drop_col_maybe(df, cols_drop_or_not, added_feature_cols, args)\n",
    "            \n",
    "\n",
    "            # Split train/eval by date\n",
    "            (self._lgb_train, self._lgb_eval, df) = self._split_data(df, **args)\n",
    "\n",
    "            res = (\n",
    "                self._lgb_train,\n",
    "                self._lgb_eval,\n",
    "                df,\n",
    "                self._cat_f,\n",
    "                added_feature_cols,\n",
    "                shift,\n",
    "            )\n",
    "\n",
    "            yield res\n",
    "\n",
    "    def _shift_target(self, df, shift, added_feature_cols):\n",
    "        \"\"\"\n",
    "        Shift data so that I can predict 1,2 and 3 months in advance\n",
    "        \"\"\"\n",
    "\n",
    "        f_col = \"shift_in_months\"\n",
    "\n",
    "        # assert shift < 0, \"Shift has to be less than 0\"\n",
    "\n",
    "        df[self._col_target_shifted] = (\n",
    "            df.sort_values([self._col_cfips, self._col_first_day_of_month])\n",
    "            .groupby(self._col_cfips)[self._col_target]\n",
    "            .shift(shift)\n",
    "        )\n",
    "\n",
    "        df = df.dropna(subset=self._col_target_shifted).copy()\n",
    "\n",
    "        df[f_col] = shift\n",
    "\n",
    "        added_feature_cols.append(f_col)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _create_data(self, df, added_feature_cols, args):\n",
    "        # Split logic\n",
    "\n",
    "        # Feature engineering\n",
    "        df = self._feature_engineering(df, added_feature_cols, args)\n",
    "\n",
    "        # Mape needs values over 1. When not using mape, irrelevant\n",
    "        target_multiplier = 1\n",
    "        df[self._col_target_shifted] = df[self._col_target_shifted] * target_multiplier\n",
    "\n",
    "        # Handle categorical features\n",
    "        print(df.info())\n",
    "        cat_f = [\"state\", \"county\", \"cfips\"]\n",
    "        for c in cat_f:\n",
    "            print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print('Changing ´{}´ to catgeory'.format(c))\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "            print(df[c].cat.codes)\n",
    "        print(df.info())\n",
    "\n",
    "        # print('added_feature_cols', added_feature_cols)\n",
    "        features = (\n",
    "            [\n",
    "                \"median_hh_inc\",\n",
    "                \"pct_bb\",\n",
    "                \"pct_college\",\n",
    "                \"pct_foreign_born\",\n",
    "                \"pct_it_workers\",\n",
    "            ]\n",
    "            + added_feature_cols\n",
    "            # + cat_f\n",
    "        )\n",
    "        \n",
    "        return (df, features, cat_f)\n",
    "\n",
    "    def _feature_engineering(self, df, added_feature_cols, args):\n",
    "        \"\"\"\n",
    "        0 means feature will not be included\n",
    "        \"\"\"\n",
    "        # Time arrow\n",
    "        df = time_arrow(df, added_feature_cols)\n",
    "\n",
    "        # Add previous target info\n",
    "        df = add_feature_targets_history(df, added_feature_cols, **args)\n",
    "\n",
    "        print('------------------------------------')\n",
    "        print('before add_feature_targets_groupby_stats', added_feature_cols)\n",
    "        # Add std, mean, median for cfips, county, state - using only train data\n",
    "        df = add_feature_targets_groupby_stats(\n",
    "            df, added_feature_cols, **args\n",
    "        )\n",
    "        print('after add_feature_targets_groupby_stats df.columns', df.columns)\n",
    "        print('after add_feature_targets_groupby_stats added_feature_cols', added_feature_cols)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _split_data(self, df, drop_na, outlier_multiplier, **kwargs):\n",
    "        # df = df.copy()\n",
    "        if drop_na > 0.5:\n",
    "            df = df.dropna()\n",
    "\n",
    "        # Split train/val split according to dates\n",
    "        dates_train, dates_val = split_dates(df)\n",
    "        train = df[df[\"first_day_of_month\"].isin(dates_train)]\n",
    "        val = df[df[\"first_day_of_month\"].isin(dates_val)]\n",
    "\n",
    "        # print('train', train)\n",
    "        \n",
    "        # Remove outliers\n",
    "        train = remove_outliers(train, outlier_multiplier)\n",
    "        \n",
    "        print('self._features', self._features)\n",
    "        \n",
    "        x_train = train[self._features]\n",
    "        y_train = train[self._col_target_shifted]\n",
    "\n",
    "        x_val = val[self._features]\n",
    "        y_val = val[self._col_target_shifted]\n",
    "\n",
    "        print('??????????????????????????')\n",
    "        print('self._cat_f', self._cat_f)\n",
    "        # lgb_train = lgb.Dataset(x_train, y_train)\n",
    "        lgb_train = lgb.Dataset(\n",
    "            x_train, y_train, \n",
    "            categorical_feature=self._cat_f, \n",
    "            free_raw_data=False\n",
    "        )\n",
    "        lgb_eval = lgb.Dataset(x_val, y_val, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "        return (lgb_train, lgb_eval, df)\n",
    "\n",
    "    def _drop_col_maybe(self, df, cols_drop_or_not: list, added_feature_cols: list, args: dict):\n",
    "        cols_to_drop = []\n",
    "        cols_bay_val = []\n",
    "        all_pairs = []\n",
    "        for col in cols_drop_or_not:\n",
    "            all_pairs.append([col, args[col]])\n",
    "            if args[col] < 0.5:\n",
    "                drop_col = col.replace('_activated', '')\n",
    "                cols_to_drop.append(drop_col)\n",
    "                cols_bay_val.append(args[col])\n",
    "                \n",
    "                # Update feature list as well\n",
    "                idx = added_feature_cols.index(drop_col)\n",
    "                del added_feature_cols[idx]\n",
    "                \n",
    "                # Update cat_f list as well\n",
    "                try:\n",
    "                    idx = self._cat_f.index(drop_col)\n",
    "                    del self._cat_f[idx]\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"df.columns\", df.columns)\n",
    "        print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "        print(\"cols_to_drop\", cols_to_drop)\n",
    "        print('--------------')\n",
    "        print(\"cols_bay_val\", cols_bay_val)\n",
    "        print('--------------')\n",
    "        print(\"all_pairs\", all_pairs)\n",
    "        print('--------------')\n",
    "\n",
    "        # return df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dcff9-fbff-4fd2-8f60-feb3de970e63",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGBMBayesian:\n",
    "    def __init__(self, PrepareData, df_train):\n",
    "        self.prepare_data: PrepareData = PrepareData(df_train)\n",
    "\n",
    "        self._debug_different_shifts: dict = {}\n",
    "\n",
    "        # Shows if there are columns which are not optimized by bayesian.\n",
    "        self._warn_bayes_not_optimize_columns = True\n",
    "\n",
    "    def optimize(self, bounds, init_points=1, n_iter=1, xi=1e-4):\n",
    "        # Initialize the Bayesian Optimization\n",
    "        self.optimizer = BayesianOptimization(self.train_model, bounds, random_state=0)\n",
    "\n",
    "        # af = bayes.UtilityFunction('poi', kappa=100)\n",
    "        acquisition_function = bayes.UtilityFunction(kind=\"ei\", xi=xi)\n",
    "\n",
    "        # Run the optimization\n",
    "        self.optimizer.maximize(\n",
    "            # init_points=100, n_iter=100\n",
    "            init_points=init_points,\n",
    "            n_iter=n_iter,\n",
    "            acquisition_function=acquisition_function,\n",
    "        )\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_params = self.optimizer.max[\"params\"]\n",
    "        print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "    def train_model(\n",
    "        self,\n",
    "        shifts=[-1, -2, -3],\n",
    "        early_stopping=True,\n",
    "        log_evaluation=True,\n",
    "        num_threads=6,\n",
    "        # verbose=2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # print(\"kwargs\", kwargs)\n",
    "\n",
    "        saved_args = locals()\n",
    "        saved_args.update(saved_args[\"kwargs\"])\n",
    "        del saved_args[\"self\"], saved_args[\"kwargs\"]\n",
    "        scores = []\n",
    "\n",
    "        # print('train_model - saved_args', saved_args)\n",
    "\n",
    "        for (\n",
    "            lgb_train,\n",
    "            lgb_eval,\n",
    "            df,\n",
    "            cat_f,\n",
    "            features,\n",
    "            shift,\n",
    "        ) in self.prepare_data.prepare_data_for_model(saved_args):\n",
    "            self._check_bayes_optimize_columns(saved_args, features)\n",
    "\n",
    "            params = {\n",
    "                \"boosting_type\": \"gbdt\",\n",
    "                \"objective\": \"mae\",\n",
    "                \"metric\": \"None\",\n",
    "                \"first_metric_only\": True,\n",
    "                # \"num_iterations\": int(num_iterations),\n",
    "                \"num_leaves\": int(kwargs[\"num_leaves\"]),\n",
    "                \"learning_rate\": kwargs[\"learning_rate\"],\n",
    "                \"subsample\": kwargs[\"subsample\"],\n",
    "                \"colsample_bytree\": kwargs[\"colsample_bytree\"],\n",
    "                \"reg_alpha\": kwargs[\"reg_alpha\"],\n",
    "                \"reg_lambda\": kwargs[\"reg_lambda\"],\n",
    "                # \"verbose\": verbose,\n",
    "                \"num_threads\": num_threads,\n",
    "            }\n",
    "\n",
    "            callbacks = []\n",
    "            # Stop earlier if no changes\n",
    "            if early_stopping:\n",
    "                callbacks.append(lgb.early_stopping(100))\n",
    "\n",
    "            # Log every X-th line\n",
    "            if log_evaluation:\n",
    "                callbacks.append(lgb.log_evaluation(100))\n",
    "\n",
    "            # Can be used to supress warnings\n",
    "            # with warnings.catch_warnings():\n",
    "            #     warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "            gbm = lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                num_boost_round=int(kwargs[\"num_iterations\"]),\n",
    "                callbacks=callbacks,\n",
    "                valid_sets=[lgb_eval],\n",
    "                feval=smape,\n",
    "            )\n",
    "\n",
    "            scores.append(gbm.best_score[\"valid_0\"][\"smape\"])\n",
    "\n",
    "            pred_eval = gbm.predict(lgb_eval.data)\n",
    "\n",
    "            res = {\n",
    "                \"lgb_train_data\": lgb_train.data,\n",
    "                \"lgb_train_label\": lgb_train.label,\n",
    "                \"lgb_eval_data\": lgb_eval.data,\n",
    "                \"lgb_eval_label\": lgb_eval.label,\n",
    "                \"df\": df.copy(),\n",
    "                \"cat_f\": cat_f,\n",
    "                \"features\": features,\n",
    "                \"pred_eval\": pred_eval,\n",
    "                \"gbm\": deepcopy(gbm),\n",
    "            }\n",
    "\n",
    "            self._debug_different_shifts[shift] = res\n",
    "\n",
    "        return -np.array(scores).mean()\n",
    "\n",
    "    def per_case_error(self, shift):\n",
    "        t0 = self._debug_different_shifts[shift]\n",
    "        data_eval = t0[\"lgb_eval_data\"].copy()\n",
    "        data_eval[\"target\"] = t0[\"lgb_eval_label\"]\n",
    "        data_eval[\"pred\"] = t0[\"pred_eval\"]\n",
    "        data_eval[\"smape\"] = data_eval.apply(\n",
    "            lambda x: smape(x[\"pred\"], x[\"target\"])[1], axis=1\n",
    "        )\n",
    "\n",
    "        cols_to_copy = [\"row_id\", \"cfips\", \"first_day_of_month\"]\n",
    "        data_eval[cols_to_copy] = t0[\"df\"][cols_to_copy]\n",
    "\n",
    "        return data_eval\n",
    "\n",
    "    def _check_bayes_optimize_columns(self, saved_args, features):\n",
    "        if self._warn_bayes_not_optimize_columns:\n",
    "            columns_no_bayes = list(set(features) - saved_args.keys())\n",
    "\n",
    "            if len(columns_no_bayes) > 0:\n",
    "                print(\">>>>>>>>>>>>>>>>>\")\n",
    "                print(\n",
    "                    f\"{bcolors.WARNING}Following columns are not optimized by bayesian:{bcolors.ENDC}\"\n",
    "                )\n",
    "                # print('Following columns are not optimized by bayesian:')\n",
    "                print(columns_no_bayes)\n",
    "                print(\">>>>>>>>>>>>>>>>>\")\n",
    "\n",
    "            self._warn_bayes_not_optimize_columns = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f34cd7-d4df-47c5-adff-f1e38bcd1883",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d89fc8-2d6f-4cc1-8a48-66046b89c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_mapping = {\n",
    "#     \"state\",\n",
    "#     \"pct_college\",\n",
    "#     \"shift_in_months\",\n",
    "#     \"pct_it_workers\",\n",
    "#     \"pct_bb\",\n",
    "#     \"pct_foreign_born\",\n",
    "#     \"median_hh_inc\",\n",
    "#     # \"month\",\n",
    "#     \"cfips\",\n",
    "#     \"county\",\n",
    "#     # New features\n",
    "#     \"time_arrow\",\n",
    "#     ## Stats\n",
    "#     \"target_cfips_std\",\n",
    "#     \"target_cfips_median\",\n",
    "#     \"target_county_median\",\n",
    "#     \"target_county_mean\",\n",
    "#     \"target_state_mean\",\n",
    "#     \"target_cfips_mean\",\n",
    "#     \"target_county_std\",\n",
    "#     \"target_state_median\",\n",
    "#     \"target_state_std\",\n",
    "#     ## Roll\n",
    "#     \"target_mean_rolling_1_activated\",\n",
    "#     \"target_mean_rolling_2_activated\",\n",
    "#     \"target_mean_rolling_3_activated\",\n",
    "#     \"target_mean_rolling_4_activated\",\n",
    "#     ## Shift \n",
    "#     \"target_shift_1_activated\",\n",
    "#     \"target_shift_2_activated\",\n",
    "#     \"target_shift_3_activated\",\n",
    "#     \"target_shift_4_activated\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261ec27-06de-4fc8-af36-cc4617df8f1b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "binary_bounds = (0, 1)\n",
    "\n",
    "min_window = 1\n",
    "max_window = 10\n",
    "\n",
    "min_shift = 1\n",
    "max_shift = 10\n",
    "\n",
    "bounds = {\n",
    "    # Model hyperparameters\n",
    "    \"num_leaves\": (5, 100),\n",
    "    \"num_iterations\": (50, 1000),\n",
    "    \"learning_rate\": (0.01, 0.5),\n",
    "    \"subsample\": (0.1, 1),\n",
    "    \"colsample_bytree\": (0.1, 1),\n",
    "    \"reg_alpha\": (0, 10),\n",
    "    \"reg_lambda\": (0, 10),\n",
    "    # Disable/Enable existing features\n",
    "    \"state\": binary_bounds,\n",
    "    \"pct_college\": binary_bounds,\n",
    "    \"shift_in_months\": binary_bounds,\n",
    "    \"pct_it_workers\": binary_bounds,\n",
    "    \"pct_bb\": binary_bounds,\n",
    "    \"pct_foreign_born\": binary_bounds,\n",
    "    \"median_hh_inc\": binary_bounds,\n",
    "    \"month\": binary_bounds,\n",
    "    \"cfips\": binary_bounds,\n",
    "    \"county\": binary_bounds,\n",
    "    # Drop all rows having NaN\n",
    "    \"drop_na\": binary_bounds,\n",
    "    # New features\n",
    "    ## Simulates time from 0(oldest) to 1 newest\n",
    "    \"time_arrow\": binary_bounds,\n",
    "    ## Rolling window bounds\n",
    "    \"target_mean_rolling_1\": (min_window, max_window),\n",
    "    \"target_mean_rolling_2\": (min_window, max_window),\n",
    "    \"target_mean_rolling_3\": (min_window, max_window),\n",
    "    \"target_mean_rolling_4\": (min_window, max_window),\n",
    "    ## Shift bounds\n",
    "    \"target_shift_1\": (min_shift, max_shift),\n",
    "    \"target_shift_2\": (min_shift, max_shift),\n",
    "    \"target_shift_3\": (min_shift, max_shift),\n",
    "    \"target_shift_4\": (min_shift, max_shift),\n",
    "    ## Stats\n",
    "    \"target_cfips_std\": binary_bounds,\n",
    "    \"target_cfips_median\": binary_bounds,\n",
    "    \"target_county_median\": binary_bounds,\n",
    "    \"target_county_mean\": binary_bounds,\n",
    "    \"target_state_mean\": binary_bounds,\n",
    "    \"target_cfips_mean\": binary_bounds,\n",
    "    \"target_county_std\": binary_bounds,\n",
    "    \"target_state_median\": binary_bounds,\n",
    "    \"target_state_std\": binary_bounds,\n",
    "    ## Smaller value less outliers\n",
    "    \"outlier_multiplier\": (0.1, 100),\n",
    "    # Disable/Enable features\n",
    "    ## Pre-existing features\n",
    "    \"enabled_state\": binary_bounds,\n",
    "    \"enabled_pct_college\": binary_bounds,\n",
    "    \"enabled_shift_in_months\": binary_bounds,\n",
    "    \"enabled_pct_it_workers\": binary_bounds,\n",
    "    \"enabled_pct_bb\": binary_bounds,\n",
    "    \"enabled_pct_foreign_born\": binary_bounds,\n",
    "    \"enabled_median_hh_inc\": binary_bounds,\n",
    "    \"enabled_month\": binary_bounds,\n",
    "    \"enabled_cfips\": binary_bounds,\n",
    "    \"enabled_county\": binary_bounds,\n",
    "    ## Stats\n",
    "    \"enabled_target_cfips_std\": binary_bounds,\n",
    "    \"enabled_target_cfips_median\": binary_bounds,\n",
    "    \"enabled_target_county_median\": binary_bounds,\n",
    "    \"enabled_target_county_mean\": binary_bounds,\n",
    "    \"enabled_target_state_mean\": binary_bounds,\n",
    "    \"enabled_target_cfips_mean\": binary_bounds,\n",
    "    \"enabled_target_county_std\": binary_bounds,\n",
    "    \"enabled_target_state_median\": binary_bounds,\n",
    "    \"enabled_target_state_std\": binary_bounds,\n",
    "    ## Roll\n",
    "    \"enabled_target_mean_rolling_1\": binary_bounds,\n",
    "    \"enabled_target_mean_rolling_2\": binary_bounds,\n",
    "    \"enabled_target_mean_rolling_3\": binary_bounds,\n",
    "    \"enabled_target_mean_rolling_4\": binary_bounds,\n",
    "    ## Shift \n",
    "    \"enabled_target_shift_1\": binary_bounds,\n",
    "    \"enabled_target_shift_2\": binary_bounds,\n",
    "    \"enabled_target_shift_3\": binary_bounds,\n",
    "    \"enabled_target_shift_4\": binary_bounds,\n",
    "}\n",
    "\n",
    "# lightgbm_bayesian = LightGBMBayesian(PrepareData, df_train)\n",
    "# lightgbm_bayesian.optimize(bounds, init_points=1, n_iter=1, xi=1e-2)\n",
    "\n",
    "# save_bayesian_results(\n",
    "#     \"smape\", lightgbm_bayesian.optimizer.res, lightgbm_bayesian.optimizer.max\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535f9b0-18e2-4ad9-a5fb-46e4db0e9f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_bayesian_results('smape', lightgbm_bayesian.optimizer.res, lightgbm_bayesian.optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1bc8f-4bec-4c9b-80f0-38009014832f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = read_pkl(\"../data/bayesian_runs/smape/2023-02-17_05-12-23_2.7925/best.pkl\")\n",
    "t_res = read_pkl(\"../data/bayesian_runs/smape/2023-02-17_05-12-23_2.7925/result.pkl\")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf697c2-4063-4c2d-957c-7d4c1332dc34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = np.array([x[\"target\"] for x in t_res])\n",
    "df_t0 = pd.DataFrame(t0 * -1, columns=[\"bayes_target\"])\n",
    "ax = df_t0.plot()\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45442dc-260a-4157-b019-527469f0be7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lightgbm_bayesian.optimizer.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273e6d6-e438-409d-8a8f-48f3e3c7868a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lightgbm_bayesian = LightGBMBayesian(PrepareData, df_train)\n",
    "start = time.time()\n",
    "k = lightgbm_bayesian.train_model(**t[\"params\"])\n",
    "debug_data = lightgbm_bayesian._debug_different_shifts\n",
    "s1 = debug_data[-1]\n",
    "df = s1[\"df\"]\n",
    "# debug_data[1][4]\n",
    "print(\"SMAPE ==\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8d5c6-44ad-437e-85e5-ba5b3323bd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ccf69-40cd-4cf5-a4ab-eea4a0a84f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c2fdd-b2c7-4792-8149-8b233f46f70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(s1[\"gbm\"], max_num_features=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce1709-7f2b-474a-835e-6cd4ff44cb6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb.Dataset(s1[\"lgb_eval_data\"], s1[\"lgb_eval_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4942b7-e0ef-400e-b81f-ad0b9d074a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_eval = lightgbm_bayesian.per_case_error(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffa63c-cc39-4333-a8ce-86ce21620ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_eval[[\"target\", \"pred\", \"smape\"]].sort_values(\"smape\").reset_index(\n",
    "    drop=True\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40feedd-81e1-4756-92c5-321675ba0033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888361ba-ac0d-48ea-a537-942742bc672f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1[\"lgb_eval_data\"].loc[25:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b69b4-4724-4324-a630-3f0caef061ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1[\"df\"].loc[25:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1122701-865b-4f7a-a0b2-c221e6ae635c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_eval.sort_values(\"smape\").tail(50)[\"cfips\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf07580-191e-4e44-a795-77f6c581bef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_eval[data_eval[\"cfips\"] == 56033].set_index(\"first_day_of_month\")[\n",
    "    [\"target\", \"pred\", \"smape\"]\n",
    "].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da4b764-2210-45fe-8957-50e31df2d098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_eval[data_eval[\"cfips\"] == 56033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62c23b-f60e-4ff2-a9a7-3cc1eae61356",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     \"colsample_bytree\": 0.915899949299061,\n",
    "#     \"drop_na\": 0.7740473326986388,\n",
    "#     \"learning_rate\": 0.17324112449403456,\n",
    "#     \"median_hh_inc\": 0.08110138998799676,\n",
    "#     \"num_iterations\": 436.87911284311696,\n",
    "#     \"num_leaves\": 27.06224350623956,\n",
    "#     \"pct_bb\": 0.13248763475798297,\n",
    "#     \"pct_college\": 0.05342718178682526,\n",
    "#     \"pct_foreign_born\": 0.7255943642105788,\n",
    "#     \"pct_it_workers\": 0.011427458625031028,\n",
    "#     \"reg_alpha\": 7.705807485027762,\n",
    "#     \"reg_lambda\": 1.4694664540037505,\n",
    "#     \"shift_in_months\": 0.07952208258675575,\n",
    "#     \"state\": 0.08960303423860538,\n",
    "#     \"subsample\": 0.704843026618523,\n",
    "#     \"target_mean_rolling_1\": 2.4536720985284477,\n",
    "#     \"target_mean_rolling_2\": 4.205394666800984,\n",
    "#     \"target_mean_rolling_3\": 5.573687913239169,\n",
    "#     \"target_mean_rolling_4\": 8.605511738287937,\n",
    "#     \"target_mean_rolling_5\": 0.7270442627113283,\n",
    "#     \"target_mean_rolling_7\": 0.27032790523871464,\n",
    "#     \"target_shift_1\": 1.314827992911276,\n",
    "#     \"target_shift_2\": 0.5537432042119794,\n",
    "#     \"target_shift_3\": 3.015986344809425,\n",
    "#     \"target_shift_4\": 2.6211814923967824,\n",
    "# }\n",
    "# lightgbm_bayesian = LightGBMBayesian(PrepareData, df_train)\n",
    "# k = lightgbm_bayesian.train_model(**params, return_data=False)\n",
    "# debug_data = lightgbm_bayesian._debug_different_shifts\n",
    "# # debug_data[1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3b070-4c3f-4ce6-bd6e-46479821a51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lgb.plot_importance(debug_data[-1][\"gbm\"], max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f199d-617e-4c87-b975-b37716b894ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lgb.create_tree_digraph(debug_data[-1][\"gbm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c08c9a-e574-4487-a518-9d8db0b7f5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4a204-0178-4b70-a819-8b9155014dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_data = lightgbm_bayesian._debug_different_shifts\n",
    "all_pred = []\n",
    "all_eval = []\n",
    "\n",
    "for values in debug_data.values():\n",
    "    all_eval.extend(list(values[\"lgb_eval\"].label))\n",
    "    all_pred.extend(list(values[\"pred_eval\"]))\n",
    "\n",
    "all_pred = np.array(all_pred)\n",
    "\n",
    "myobject = SimpleNamespace()  # myobject = {}\n",
    "myobject.label = np.array(all_eval)\n",
    "all_eval = myobject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230dae5-a64d-422c-96df-1f15bcfae1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smape(all_pred, all_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1cb1b-6434-48d9-9648-6f9c435c8bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_eval.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0855bc9-2e52-4b0a-bf4d-bb323adb147d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d9cf0-c665-462b-96a3-fa7e04c180fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(all_eval.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4ad78-9973-42ac-9d38-750643f3329b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d55ab-109a-46c4-beb4-ee720c4ed26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smape(values[\"pred_eval\"], values[\"lgb_eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b76fea-c4b1-4563-855b-5d5abb62de19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smape(all_pred, all_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5da84f-d067-4827-a27e-875c87942700",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85539e-1444-4b4f-8c10-822cee7731fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7cba03-368d-4daa-967a-6e222f418f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = df_train.copy()\n",
    "idx = y_train.index\n",
    "\n",
    "cols = x_train.columns.difference(df_train.columns)\n",
    "cols = x_train.columns.intersection(cols)\n",
    "t[cols] = x_train[cols].copy()\n",
    "\n",
    "t.loc[idx, \"target\"] = (y_train - target_const).values\n",
    "t.loc[idx, \"pred\"] = gbm.predict(x_train) - target_const\n",
    "t = t.loc[idx]\n",
    "t[\"diff\"] = abs(t[\"target\"] - t[\"pred\"])\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "t[\"relative\"] = t[\"diff\"] / t[\"pred\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99894c95-7057-452e-93e8-22291f9552cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359b1fd-6b6c-4561-b06a-acb0ca25925e",
   "metadata": {},
   "source": [
    "### Column mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda2a9c-3c75-4fc7-8700-8884a9f83b40",
   "metadata": {},
   "source": [
    "##### Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30b307d3-aabc-4575-8fdb-ea166e497f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# binary_bounds = (0, 1)\n",
    "\n",
    "# min_window = 1\n",
    "# max_window = 10\n",
    "\n",
    "# min_shift = 1\n",
    "# max_shift = 10\n",
    "\n",
    "# bounds = {\n",
    "#     # Model hyperparameters\n",
    "#     \"num_leaves\": (5, 100),\n",
    "#     \"num_iterations\": (50, 1000),\n",
    "#     \"learning_rate\": (0.01, 0.5),\n",
    "#     \"subsample\": (0.1, 1),\n",
    "#     \"colsample_bytree\": (0.1, 1),\n",
    "#     \"reg_alpha\": (0, 10),\n",
    "#     \"reg_lambda\": (0, 10),\n",
    "#     # Disable/Enable existing features\n",
    "#     \"state\": binary_bounds,\n",
    "#     \"pct_college\": binary_bounds,\n",
    "#     \"shift_in_months\": binary_bounds,\n",
    "#     \"pct_it_workers\": binary_bounds,\n",
    "#     \"pct_bb\": binary_bounds,\n",
    "#     \"pct_foreign_born\": binary_bounds,\n",
    "#     \"median_hh_inc\": binary_bounds,\n",
    "#     \"month\": binary_bounds,\n",
    "#     \"cfips\": binary_bounds,\n",
    "#     \"county\": binary_bounds,\n",
    "#     # Drop all rows having NaN\n",
    "#     \"drop_na\": binary_bounds,\n",
    "#     # New features\n",
    "#     ## Simulates time from 0(oldest) to 1 newest\n",
    "#     \"time_arrow\": binary_bounds,\n",
    "#     ## Rolling window bounds\n",
    "#     \"target_mean_rolling_1\": (min_window, max_window),\n",
    "#     \"target_mean_rolling_2\": (min_window, max_window),\n",
    "#     \"target_mean_rolling_3\": (min_window, max_window),\n",
    "#     \"target_mean_rolling_4\": (min_window, max_window),\n",
    "#     ## Shift bounds\n",
    "#     \"target_shift_1\": (min_shift, max_shift),\n",
    "#     \"target_shift_2\": (min_shift, max_shift),\n",
    "#     \"target_shift_3\": (min_shift, max_shift),\n",
    "#     \"target_shift_4\": (min_shift, max_shift),\n",
    "#     ## Stats\n",
    "#     \"target_cfips_std\": binary_bounds,\n",
    "#     \"target_cfips_median\": binary_bounds,\n",
    "#     \"target_county_median\": binary_bounds,\n",
    "#     \"target_county_mean\": binary_bounds,\n",
    "#     \"target_state_mean\": binary_bounds,\n",
    "#     \"target_cfips_mean\": binary_bounds,\n",
    "#     \"target_county_std\": binary_bounds,\n",
    "#     \"target_state_median\": binary_bounds,\n",
    "#     \"target_state_std\": binary_bounds,\n",
    "#     ## Smaller value less outliers\n",
    "#     \"outlier_multiplier\": (0.1, 100),\n",
    "#     # Disable/Enable features\n",
    "#     ## Pre-existing features\n",
    "#     \"enabled_state\": binary_bounds,\n",
    "#     \"enabled_pct_college\": binary_bounds,\n",
    "#     \"enabled_shift_in_months\": binary_bounds,\n",
    "#     \"enabled_pct_it_workers\": binary_bounds,\n",
    "#     \"enabled_pct_bb\": binary_bounds,\n",
    "#     \"enabled_pct_foreign_born\": binary_bounds,\n",
    "#     \"enabled_median_hh_inc\": binary_bounds,\n",
    "#     \"enabled_month\": binary_bounds,\n",
    "#     \"enabled_cfips\": binary_bounds,\n",
    "#     \"enabled_county\": binary_bounds,\n",
    "#     ## Stats\n",
    "#     \"enabled_target_cfips_std\": binary_bounds,\n",
    "#     \"enabled_target_cfips_median\": binary_bounds,\n",
    "#     \"enabled_target_county_median\": binary_bounds,\n",
    "#     \"enabled_target_county_mean\": binary_bounds,\n",
    "#     \"enabled_target_state_mean\": binary_bounds,\n",
    "#     \"enabled_target_cfips_mean\": binary_bounds,\n",
    "#     \"enabled_target_county_std\": binary_bounds,\n",
    "#     \"enabled_target_state_median\": binary_bounds,\n",
    "#     \"enabled_target_state_std\": binary_bounds,\n",
    "#     ## Roll\n",
    "#     \"enabled_target_mean_rolling_1\": binary_bounds,\n",
    "#     \"enabled_target_mean_rolling_2\": binary_bounds,\n",
    "#     \"enabled_target_mean_rolling_3\": binary_bounds,\n",
    "#     \"enabled_target_mean_rolling_4\": binary_bounds,\n",
    "#     ## Shift \n",
    "#     \"enabled_target_shift_1\": binary_bounds,\n",
    "#     \"enabled_target_shift_2\": binary_bounds,\n",
    "#     \"enabled_target_shift_3\": binary_bounds,\n",
    "#     \"enabled_target_shift_4\": binary_bounds,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc6050c2-a5ac-4bd0-b459-47029dc93ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cell_vars(offset=0):\n",
    "    \n",
    "    def filter_feature_class_only(result):\n",
    "        feature_class_only = {}\n",
    "        for key, val in result.items():\n",
    "            if str(type(val)) == \"<class '__main__.Feature'>\":\n",
    "                feature_class_only.update({key:val})\n",
    "                \n",
    "        return feature_class_only\n",
    "    \n",
    "    import io\n",
    "    from contextlib import redirect_stdout\n",
    "\n",
    "    ipy = get_ipython()\n",
    "    out = io.StringIO()\n",
    "\n",
    "    with redirect_stdout(out):\n",
    "        ipy.magic(\"history {0}\".format(ipy.execution_count - offset))\n",
    "\n",
    "    #process each line...\n",
    "    x = out.getvalue().replace(\" \", \"\").split(\"\\n\")\n",
    "    x = [a.split(\"=\")[0] for a in x if \"=\" in a] #all of the variables in the cell\n",
    "    g = globals()\n",
    "    result = {k:g[k] for k in x if k in g}\n",
    "    \n",
    "    feature_class_only = filter_feature_class_only(result)\n",
    "    \n",
    "    if len(feature_class_only) == 0:\n",
    "        raise ValueError('None found. Maybe you changed name AGAIN?')\n",
    "    \n",
    "    return feature_class_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77655050-059a-460d-82d0-d2e93d49bb15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataFrame(TraitType):\n",
    "    \"\"\"A trait for pd.DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    info_text = \"pd.DataFrame\"\n",
    "    \n",
    "    def validate(self, obj, value):\n",
    "        if type(value) == pd.DataFrame:\n",
    "            return value\n",
    "        self.error(obj, value)\n",
    "\n",
    "\n",
    "\n",
    "class Feature(HasTraits):\n",
    "    # During init\n",
    "\n",
    "    name = Unicode()\n",
    "    f = Callable()\n",
    "\n",
    "    bound = Tuple(Float(), Float())\n",
    "\n",
    "    enabled_bounds = List(bound, minlen=1)\n",
    "    params_bounds = List(bound)\n",
    "    \n",
    "    # After init\n",
    "\n",
    "    _enabled_dict = Dict(key_trait=Unicode(), value_trait=bound)\n",
    "    _params_dict = Dict(key_trait=Unicode(), value_trait=bound)\n",
    "    \n",
    "    relations = Dict(key_trait=Unicode(), value_trait=Unicode())\n",
    "    \n",
    "    _df_enabled_params = DataFrame()\n",
    "    # _df_enabled_params = Dict()\n",
    "\n",
    "    def __init__(self, name, f, df, enabled_bounds=None, params_bounds=None):\n",
    "        self.name = name\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "        if enabled_bounds:\n",
    "            self.enabled_bounds = enabled_bounds\n",
    "        if params_bounds:\n",
    "            self.params_bounds = params_bounds\n",
    "\n",
    "        self._make_enable_and_params_dict()\n",
    "        self._make_df_enable_params()\n",
    "\n",
    "    def get_bounds_for_optimizer(self):\n",
    "        return {**self._enabled_dict, **self._params_dict}\n",
    "    \n",
    "    def get_relations(self):\n",
    "        return self._relations\n",
    "    \n",
    "    def get_enabled_dict(self):\n",
    "        return self._enabled_dict\n",
    "    \n",
    "    def get_params_dict(self):\n",
    "        return self._params_dict\n",
    "\n",
    "    def get_df_enable_params(self):\n",
    "        return self._df_enabled_params\n",
    "    \n",
    "    \n",
    "    def _set_relations(self, d):\n",
    "        self._relations = d\n",
    "        \n",
    "        \n",
    "    def _make_df_enable_params(self):\n",
    "        d = self.get_relations()\n",
    "        relation_original = d\n",
    "        relation_flipped = {v: k for k, v in d.items()}\n",
    "\n",
    "        # Get relations\n",
    "        df_relation = pd.DataFrame.from_dict(\n",
    "            {**relation_original, **relation_flipped}, orient=\"index\", columns=[\"relation\"]\n",
    "        )\n",
    "\n",
    "        # Bounds\n",
    "        df_bounds = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                **self._enabled_dict,\n",
    "                **self._params_dict,\n",
    "            },\n",
    "            orient=\"index\",\n",
    "            columns=[\"min\", \"max\"],\n",
    "        )\n",
    "\n",
    "        # Join\n",
    "        df = pd.merge(df_bounds, df_relation, 'left', left_index=True, right_index=True)\n",
    "        \n",
    "        # Add name\n",
    "        df['name'] = self.name\n",
    "        \n",
    "        self._df_enabled_params = df\n",
    "\n",
    "    def _make_enable_and_params_dict(self):\n",
    "        \"\"\"\n",
    "        Create 2 variables:\n",
    "        1) self._enabled_dict\n",
    "        2) self._params_dict\n",
    "        \"\"\"\n",
    "        self._enabled_keys = []\n",
    "        self._params_keys = []\n",
    "        for kind in [\"enabled\", \"params\"]:\n",
    "            # print('self.enabled_bounds', self.enabled_bounds)\n",
    "            bounds = getattr(self, \"{}_bounds\".format(kind))\n",
    "\n",
    "            d = {}\n",
    "            key_name = \"{kind}_{name}_{idx}\"\n",
    "            for idx, bound in enumerate(bounds):\n",
    "                if len(bounds) < 2:\n",
    "                    idx = \"\"\n",
    "                    self._params_keys.append(\n",
    "                        key_name.format(kind=\"params\", name=self.name, idx=idx)\n",
    "                    )\n",
    "                key = key_name.format(kind=kind, name=self.name, idx=idx)\n",
    "                d[key] = bound\n",
    "\n",
    "                getattr(self, \"_{}_keys\".format(kind)).append(key)\n",
    "\n",
    "            setattr(self, \"_{}_dict\".format(kind), d)\n",
    "        # Set relations for all\n",
    "        d = dict(zip(self._enabled_keys, self._params_keys))\n",
    "        self._set_relations(d)\n",
    "        \n",
    "        \n",
    "    @default(\"enabled_bounds\")\n",
    "    def _default_value(self):\n",
    "        return [(1, 1)]\n",
    "\n",
    "    @validate(\"params_bounds\")\n",
    "    def _valid_params_bounds(self, proposal):\n",
    "        params_bounds = proposal[\"value\"]\n",
    "\n",
    "        if params_bounds is not None:\n",
    "            len_params_bounds = len(params_bounds)\n",
    "            len_enabled_bounds = len(self.enabled_bounds)\n",
    "            if len_params_bounds != len_enabled_bounds:\n",
    "                raise TraitError(\n",
    "                    \"If defined, ´params_bounds´ ({}) should match in length with ´enabled_bounds´ ({})\".format(\n",
    "                        len_params_bounds, len_enabled_bounds\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return proposal[\"value\"]\n",
    "\n",
    "    @validate(\"enabled_dict\")\n",
    "    def _valid_enabled_dict(self, proposal):\n",
    "        for key, value in proposal[\"value\"].items():\n",
    "            bound_min = value[0]\n",
    "            bound_max = value[1]\n",
    "\n",
    "            if (bound_min < 0 or bound_min > 1) or (bound_max < 0 or bound_max > 1):\n",
    "                raise TraitError(\n",
    "                    \"For key ´{}´ bound values have to be between 0 and 1 (both included). Values given: ´{}´\".format(\n",
    "                        key, value\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if bound_min > bound_max:\n",
    "                raise TraitError(\n",
    "                    \"Bound min cannot be bigger than max. Values given: ´{}´\".format(\n",
    "                        value\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return proposal[\"value\"]\n",
    "\n",
    "        \n",
    "#     def _add_self_to_all_paramss(self, all_paramss):\n",
    "#         all_paramss.update({self.name: self})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f0fd40-b2c4-4eb2-ad84-841d0ab402c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ManageFeatures(HasTraits):\n",
    "    _feature_objects = Dict(key_trait=Unicode())\n",
    "    _df_mapping = DataFrame()\n",
    "    _df_enabled_params = DataFrame()\n",
    "\n",
    "    def __init__(self, feature_objects):\n",
    "        self._feature_objects = feature_objects\n",
    "        \n",
    "        self._set_features_df_enabled_params()\n",
    "        \n",
    "    def get_features_df_enabled_params(self):\n",
    "        return self._df_enabled_params\n",
    "        \n",
    "    def get_pbounds(self):\n",
    "        df = self.get_features_df_enabled_params()\n",
    "        df['pbounds'] = list(zip(df['min'], df['max']))\n",
    "        \n",
    "        return df['pbounds'].to_dict()\n",
    "        \n",
    "    def get_mapping(self):\n",
    "        return self._df_mapping.copy()\n",
    "        \n",
    "    def set_mapping_return(self, *args, **kwargs):\n",
    "        self.set_mapping(*args, **kwargs)\n",
    "        return self.get_mapping()\n",
    "        \n",
    "    def _set_features_df_enabled_params(self, f=\"get_df_enable_params\"):\n",
    "        l = []\n",
    "        for key, value in self._feature_objects.items():\n",
    "            print()\n",
    "            l.append(getattr(self._feature_objects[key], f)())\n",
    "\n",
    "        self._df_enabled_params = self._validate_df_duplicate_index_and_concat(l)\n",
    "        \n",
    "    def set_mapping(self, pbounds: dict):\n",
    "        \n",
    "        df_enabled_params = self.get_features_df_enabled_params()\n",
    "        df_pbounds = pd.DataFrame.from_dict(pbounds, orient='index', columns=['params'])\n",
    "\n",
    "        df = pd.merge(df_enabled_params, df_pbounds, 'outer', left_index=True, right_index=True)\n",
    "        \n",
    "        assert df.shape == df.dropna().shape, 'There should be no NaN values'\n",
    "        \n",
    "        self._df_mapping = df\n",
    "        \n",
    "        # self._debug = (df_enabled_params, df_pbounds)\n",
    "        \n",
    "\n",
    "    def _validate_df_duplicate_index_and_concat(self, l):\n",
    "        rows_in_l = 0\n",
    "        for t in l:\n",
    "            rows_in_l += t.shape[0]\n",
    "\n",
    "        df = pd.concat(l)\n",
    "\n",
    "        assert (\n",
    "            rows_in_l == df.shape[0]\n",
    "        ), \"Row count should be same. You probably have a duplicate index between diferent ´Feature objects´\"\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9408ecd4-bed6-42d0-9dbd-3d593f3c992d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n"
     ]
    }
   ],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "enabled_tuple = (0, 1)\n",
    "\n",
    "params_tuple = (0,10)\n",
    "feature_target_rolling_mean = Feature(\n",
    "    \"target_rolling_mean\",\n",
    "    add_feature_targets_history,\n",
    "    df_train,\n",
    "    enabled_bounds=list(repeat(enabled_tuple, 4)),\n",
    "    params_bounds=list(repeat(params_tuple, 4)),\n",
    ")\n",
    "\n",
    "feature_target_shift_mean = Feature(\n",
    "    \"target_shift\",\n",
    "    add_feature_targets_history,\n",
    "    df_train,\n",
    "    enabled_bounds=list(repeat(enabled_tuple, 4)),\n",
    "    params_bounds=list(repeat(params_tuple, 4)),\n",
    ")\n",
    "\n",
    "# This has to be in the same cell as features\n",
    "all_features = cell_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f86447e-14d4-46ec-bafd-1aa07d17339b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__main__.ManageFeatures"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manage_features = ManageFeatures(all_features)\n",
    "# manage_features._get_all_relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b4d490f-e5dd-428c-b896-bfb513a34042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | enable... | enable... | enable... | enable... | enable... | enable... | enable... | enable... | params... | params... | params... | params... | params... | params... | params... | params... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'enabled_target_rolling_mean_0': 0.417022004702574, 'enabled_target_rolling_mean_1': 0.7203244934421581, 'enabled_target_rolling_mean_2': 0.00011437481734488664, 'enabled_target_rolling_mean_3': 0.30233257263183977, 'enabled_target_shift_0': 0.14675589081711304, 'enabled_target_shift_1': 0.0923385947687978, 'enabled_target_shift_2': 0.1862602113776709, 'enabled_target_shift_3': 0.34556072704304774, 'params_target_rolling_mean_0': 3.9676747423066994, 'params_target_rolling_mean_1': 5.3881673400335695, 'params_target_rolling_mean_2': 4.191945144032948, 'params_target_rolling_mean_3': 6.852195003967595, 'params_target_shift_0': 2.0445224973151745, 'params_target_shift_1': 8.781174363909454, 'params_target_shift_2': 0.27387593197926163, 'params_target_shift_3': 6.704675101784022}\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m0.7203   \u001b[0m | \u001b[0m0.0001144\u001b[0m | \u001b[0m0.3023   \u001b[0m | \u001b[0m0.1468   \u001b[0m | \u001b[0m0.09234  \u001b[0m | \u001b[0m0.1863   \u001b[0m | \u001b[0m0.3456   \u001b[0m | \u001b[0m3.968    \u001b[0m | \u001b[0m5.388    \u001b[0m | \u001b[0m4.192    \u001b[0m | \u001b[0m6.852    \u001b[0m | \u001b[0m2.045    \u001b[0m | \u001b[0m8.781    \u001b[0m | \u001b[0m0.2739   \u001b[0m | \u001b[0m6.705    \u001b[0m |\n",
      "{'enabled_target_rolling_mean_0': 0.921648971457627, 'enabled_target_rolling_mean_1': 0.5500971661055835, 'enabled_target_rolling_mean_2': 0.1689035389654009, 'enabled_target_rolling_mean_3': 0.6207653329165613, 'enabled_target_shift_0': 0.35668396140110614, 'enabled_target_shift_1': 0.6768136200999317, 'enabled_target_shift_2': 0.5701584424597982, 'enabled_target_shift_3': 0.43447922393257166, 'params_target_rolling_mean_0': 9.325200639609712, 'params_target_rolling_mean_1': 9.10834128371658, 'params_target_rolling_mean_2': 9.004110377496572, 'params_target_rolling_mean_3': 4.803573352689278, 'params_target_shift_0': 3.2038015656050787, 'params_target_shift_1': 1.5720401182549648, 'params_target_shift_2': 2.756793082262069, 'params_target_shift_3': 3.7772425011374255}\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m0.9216   \u001b[0m | \u001b[0m0.5501   \u001b[0m | \u001b[0m0.1689   \u001b[0m | \u001b[0m0.6208   \u001b[0m | \u001b[0m0.3567   \u001b[0m | \u001b[0m0.6768   \u001b[0m | \u001b[0m0.5702   \u001b[0m | \u001b[0m0.4345   \u001b[0m | \u001b[0m9.325    \u001b[0m | \u001b[0m9.108    \u001b[0m | \u001b[0m9.004    \u001b[0m | \u001b[0m4.804    \u001b[0m | \u001b[0m3.204    \u001b[0m | \u001b[0m1.572    \u001b[0m | \u001b[0m2.757    \u001b[0m | \u001b[0m3.777    \u001b[0m |\n",
      "=========================================================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>relation</th>\n",
       "      <th>name</th>\n",
       "      <th>pbounds</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>enabled_target_rolling_mean_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>params_target_rolling_mean_0</td>\n",
       "      <td>target_rolling_mean</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.417022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enabled_target_rolling_mean_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>params_target_rolling_mean_1</td>\n",
       "      <td>target_rolling_mean</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.720324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enabled_target_rolling_mean_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>params_target_rolling_mean_2</td>\n",
       "      <td>target_rolling_mean</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enabled_target_rolling_mean_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>params_target_rolling_mean_3</td>\n",
       "      <td>target_rolling_mean</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.302333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enabled_target_shift_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>params_target_shift_0</td>\n",
       "      <td>target_shift</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.146756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enabled_target_shift_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>params_target_shift_1</td>\n",
       "      <td>target_shift</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.092339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enabled_target_shift_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>params_target_shift_2</td>\n",
       "      <td>target_shift</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.186260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enabled_target_shift_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>params_target_shift_3</td>\n",
       "      <td>target_shift</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.345561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params_target_rolling_mean_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>enabled_target_rolling_mean_0</td>\n",
       "      <td>target_rolling_mean</td>\n",
       "      <td>(0.0, 10.0)</td>\n",
       "      <td>3.967675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params_target_rolling_mean_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>enabled_target_rolling_mean_1</td>\n",
       "      <td>target_rolling_mean</td>\n",
       "      <td>(0.0, 10.0)</td>\n",
       "      <td>5.388167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params_target_rolling_mean_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>enabled_target_rolling_mean_2</td>\n",
       "      <td>target_rolling_mean</td>\n",
       "      <td>(0.0, 10.0)</td>\n",
       "      <td>4.191945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params_target_rolling_mean_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>enabled_target_rolling_mean_3</td>\n",
       "      <td>target_rolling_mean</td>\n",
       "      <td>(0.0, 10.0)</td>\n",
       "      <td>6.852195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params_target_shift_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>enabled_target_shift_0</td>\n",
       "      <td>target_shift</td>\n",
       "      <td>(0.0, 10.0)</td>\n",
       "      <td>2.044522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params_target_shift_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>enabled_target_shift_1</td>\n",
       "      <td>target_shift</td>\n",
       "      <td>(0.0, 10.0)</td>\n",
       "      <td>8.781174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params_target_shift_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>enabled_target_shift_2</td>\n",
       "      <td>target_shift</td>\n",
       "      <td>(0.0, 10.0)</td>\n",
       "      <td>0.273876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params_target_shift_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>enabled_target_shift_3</td>\n",
       "      <td>target_shift</td>\n",
       "      <td>(0.0, 10.0)</td>\n",
       "      <td>6.704675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               min   max                       relation  \\\n",
       "enabled_target_rolling_mean_0  0.0   1.0   params_target_rolling_mean_0   \n",
       "enabled_target_rolling_mean_1  0.0   1.0   params_target_rolling_mean_1   \n",
       "enabled_target_rolling_mean_2  0.0   1.0   params_target_rolling_mean_2   \n",
       "enabled_target_rolling_mean_3  0.0   1.0   params_target_rolling_mean_3   \n",
       "enabled_target_shift_0         0.0   1.0          params_target_shift_0   \n",
       "enabled_target_shift_1         0.0   1.0          params_target_shift_1   \n",
       "enabled_target_shift_2         0.0   1.0          params_target_shift_2   \n",
       "enabled_target_shift_3         0.0   1.0          params_target_shift_3   \n",
       "params_target_rolling_mean_0   0.0  10.0  enabled_target_rolling_mean_0   \n",
       "params_target_rolling_mean_1   0.0  10.0  enabled_target_rolling_mean_1   \n",
       "params_target_rolling_mean_2   0.0  10.0  enabled_target_rolling_mean_2   \n",
       "params_target_rolling_mean_3   0.0  10.0  enabled_target_rolling_mean_3   \n",
       "params_target_shift_0          0.0  10.0         enabled_target_shift_0   \n",
       "params_target_shift_1          0.0  10.0         enabled_target_shift_1   \n",
       "params_target_shift_2          0.0  10.0         enabled_target_shift_2   \n",
       "params_target_shift_3          0.0  10.0         enabled_target_shift_3   \n",
       "\n",
       "                                              name      pbounds    params  \n",
       "enabled_target_rolling_mean_0  target_rolling_mean   (0.0, 1.0)  0.417022  \n",
       "enabled_target_rolling_mean_1  target_rolling_mean   (0.0, 1.0)  0.720324  \n",
       "enabled_target_rolling_mean_2  target_rolling_mean   (0.0, 1.0)  0.000114  \n",
       "enabled_target_rolling_mean_3  target_rolling_mean   (0.0, 1.0)  0.302333  \n",
       "enabled_target_shift_0                target_shift   (0.0, 1.0)  0.146756  \n",
       "enabled_target_shift_1                target_shift   (0.0, 1.0)  0.092339  \n",
       "enabled_target_shift_2                target_shift   (0.0, 1.0)  0.186260  \n",
       "enabled_target_shift_3                target_shift   (0.0, 1.0)  0.345561  \n",
       "params_target_rolling_mean_0   target_rolling_mean  (0.0, 10.0)  3.967675  \n",
       "params_target_rolling_mean_1   target_rolling_mean  (0.0, 10.0)  5.388167  \n",
       "params_target_rolling_mean_2   target_rolling_mean  (0.0, 10.0)  4.191945  \n",
       "params_target_rolling_mean_3   target_rolling_mean  (0.0, 10.0)  6.852195  \n",
       "params_target_shift_0                 target_shift  (0.0, 10.0)  2.044522  \n",
       "params_target_shift_1                 target_shift  (0.0, 10.0)  8.781174  \n",
       "params_target_shift_2                 target_shift  (0.0, 10.0)  0.273876  \n",
       "params_target_shift_3                 target_shift  (0.0, 10.0)  6.704675  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def black_box_function(**kwargs):\n",
    "    \"\"\"Function with unknown internals we wish to maximize.\n",
    "\n",
    "    This is just serving as an example, for all intents and\n",
    "    purposes think of the internals of this function, i.e.: the process\n",
    "    which generates its output values, as unknown.\n",
    "    \"\"\"\n",
    "    print(kwargs)\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = manage_features.get_pbounds()\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=1,\n",
    "    n_iter=1,\n",
    ")\n",
    "\n",
    "bayes_exploring_values = {\n",
    "    \"enabled_target_rolling_mean_0\": 0.417022004702574,\n",
    "    \"enabled_target_rolling_mean_1\": 0.7203244934421581,\n",
    "    \"enabled_target_rolling_mean_2\": 0.00011437481734488664,\n",
    "    \"enabled_target_rolling_mean_3\": 0.30233257263183977,\n",
    "    \"enabled_target_shift_0\": 0.14675589081711304,\n",
    "    \"enabled_target_shift_1\": 0.0923385947687978,\n",
    "    \"enabled_target_shift_2\": 0.1862602113776709,\n",
    "    \"enabled_target_shift_3\": 0.34556072704304774,\n",
    "    \"params_target_rolling_mean_0\": 3.9676747423066994,\n",
    "    \"params_target_rolling_mean_1\": 5.3881673400335695,\n",
    "    \"params_target_rolling_mean_2\": 4.191945144032948,\n",
    "    \"params_target_rolling_mean_3\": 6.852195003967595,\n",
    "    \"params_target_shift_0\": 2.0445224973151745,\n",
    "    \"params_target_shift_1\": 8.781174363909454,\n",
    "    \"params_target_shift_2\": 0.27387593197926163,\n",
    "    \"params_target_shift_3\": 6.704675101784022,\n",
    "}\n",
    "\n",
    "\n",
    "manage_features.set_mapping_return(optimizer.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d11b477-689d-43c2-826b-c1fa028950d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enabled_target_rolling_mean_0': 0.417022004702574,\n",
       " 'enabled_target_rolling_mean_1': 0.7203244934421581,\n",
       " 'enabled_target_rolling_mean_2': 0.00011437481734488664,\n",
       " 'enabled_target_rolling_mean_3': 0.30233257263183977,\n",
       " 'enabled_target_shift_0': 0.14675589081711304,\n",
       " 'enabled_target_shift_1': 0.0923385947687978,\n",
       " 'enabled_target_shift_2': 0.1862602113776709,\n",
       " 'enabled_target_shift_3': 0.34556072704304774,\n",
       " 'params_target_rolling_mean_0': 3.9676747423066994,\n",
       " 'params_target_rolling_mean_1': 5.3881673400335695,\n",
       " 'params_target_rolling_mean_2': 4.191945144032948,\n",
       " 'params_target_rolling_mean_3': 6.852195003967595,\n",
       " 'params_target_shift_0': 2.0445224973151745,\n",
       " 'params_target_shift_1': 8.781174363909454,\n",
       " 'params_target_shift_2': 0.27387593197926163,\n",
       " 'params_target_shift_3': 6.704675101784022}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e306bae-6547-4df2-a4d6-ee6258a828b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9f635-b73b-4bd1-8520-bce4ef089721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e252d713-79cc-42b2-9f04-668a3c8c7113",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Per state target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d49a6-ab91-4db2-9269-b3511a099d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dates, eval_dates = split_dates(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071d746-69a5-4799-8df5-e5b99b5939c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd4325-7c0b-441a-88bf-e5254c3caea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = df_train[df_train[\"first_day_of_month\"].isin(train_dates)]\n",
    "eval_data = df_train[df_train[\"first_day_of_month\"].isin(eval_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276400a-d5db-473d-b2de-52f81739393a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_feature_targets_groupby_stats(\n",
    "    df,\n",
    "    col_group,\n",
    "    new_col_template=\"{}_target_{}\",\n",
    "    agg_functions=[\"mean\", \"std\", \"median\"],\n",
    "):\n",
    "    t0 = df.groupby(col_group)[\"microbusiness_density\"].agg(agg_functions)\n",
    "    t0.columns = [new_col.format(col_group, x) for x in t0.columns]\n",
    "\n",
    "    df = pd.merge(df, t0, \"left\", left_on=col_group, right_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e881bcb-85aa-41dc-8038-0f95ee0c0243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_feature_targets_groupby_stats(train_data, \"state\").sort_values(\n",
    "    \"state_target_std\"\n",
    ").reset_index(drop=True)[\"state_target_std\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3ed3b-302e-4d12-9129-91a9d0fc0bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = (\n",
    "    train_data.sort_values(\"microbusiness_density\")\n",
    "    .reset_index(drop=True)[\"microbusiness_density\"]\n",
    "    .plot(logy=True)\n",
    ")\n",
    "eval_data.sort_values(\"microbusiness_density\").reset_index(drop=True)[\n",
    "    \"microbusiness_density\"\n",
    "].plot(logy=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d26eee-83a6-48da-83fc-bd69180f243e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col = \"cfips\"\n",
    "new_col = \"{}_target_{}\"\n",
    "t0 = train_data.groupby(\"cfips\")[\"microbusiness_density\"].agg(agg_functions)\n",
    "\n",
    "t0.columns = [new_col.format(col, x) for x in t0.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f47e7-09bc-4a0e-9fa5-a1089c2135a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0.sort_values(\"cfips_target_std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49efee0-960d-4f69-9ed9-3777e7d2dd48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0.sort_values(\"cfips_target_mean\").reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e214977-c578-48f5-8387-285b60ba490f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b4a7a-dcce-4614-b802-878c75820945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train[df_train[\"state\"] == \"Alabama\"][\"cfips\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036376b8-fa8d-42bd-8a08-502126804641",
   "metadata": {},
   "source": [
    "### Split dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097c95c-e45f-4920-bc53-b5193fc2f217",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dates(df):\n",
    "    \"\"\"\n",
    "    Split dates used for splitting train/test\n",
    "    \"\"\"\n",
    "    dates = np.sort(df[\"first_day_of_month\"].unique())\n",
    "    c = int(dates.shape[0] * 0.70)\n",
    "    dates_train = dates[:c]\n",
    "    dates_val = dates[c:]\n",
    "\n",
    "    return (dates_train, dates_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0082af-c160-4dcb-9d4f-bd42afab1d06",
   "metadata": {},
   "source": [
    "### Long/Lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90183797-018c-4ea6-a9bd-3bc17d146d93",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "color_scale = [(0, \"orange\"), (1, \"red\")]\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    df_boundaries,\n",
    "    lat=\"INTPTLAT\",\n",
    "    lon=\"INTPTLON\",\n",
    "    color_continuous_scale=color_scale,\n",
    "    zoom=8,\n",
    "    height=800,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1fa76-a4d3-49fc-8876-ea36b14e1992",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a172d5-fdc8-4f34-b988-1b80e04c6c97",
   "metadata": {},
   "source": [
    "#### State corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65200b17-4fda-4027-a442-35e0002843ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = (\n",
    "    df_train.groupby([\"state\", \"first_day_of_month\"])[\"microbusiness_density\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "t1 = t0.pivot_table(\"mean\", \"first_day_of_month\", \"state\").sort_index().corr()\n",
    "plt.imshow(t1.values, cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cb477-2a1d-4ae2-801c-b14c8c292ae9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ced21-2b6d-4cd1-9f2c-e350c37a5cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t5 = t1.rename_axis([\"other_state\"], axis=1).stack().rename(\"corr\").reset_index()\n",
    "t5 = t5[t5[\"state\"] != t5[\"other_state\"]]\n",
    "\n",
    "# Clean pairs of same correlations\n",
    "t5 = t5.sort_values(\"corr\").reset_index(drop=True)\n",
    "cols = [\"state\", \"other_state\"]\n",
    "t5[cols] = pd.DataFrame(np.sort(t5[cols].values, axis=1), columns=cols)\n",
    "t5 = t5.drop_duplicates()\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.01, min_samples=2).fit(t5[\"corr\"].values.reshape(-1, 1))\n",
    "\n",
    "t5[\"cluster\"] = clustering.labels_\n",
    "t5.groupby([\"cluster\"])[\"corr\"].agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5179e-7265-4fe7-956a-fd377859e95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3fb38-016d-431b-9b48-4353fa1d2822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_states = t5[abs(t5[\"corr\"]) > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8791e47-06b4-489c-ac0b-97662fce2805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = (\n",
    "    df_train[df_train[\"state\"] == \"Louisiana\"]\n",
    "    .sort_values(\"first_day_of_month\")\n",
    "    .groupby(\"first_day_of_month\")[\"microbusiness_density\"]\n",
    "    .mean()\n",
    "    .plot()\n",
    ")\n",
    "\n",
    "# ax = df_train[df_train[\"state\"] == \"Mississippi\"].sort_values(\"first_day_of_month\").groupby(\n",
    "#     \"first_day_of_month\"\n",
    "# )[\"microbusiness_density\"].mean().plot(ax=ax)\n",
    "\n",
    "t9 = (\n",
    "    df_train[df_train[\"state\"] == \"Mississippi\"]\n",
    "    .sort_values(\"first_day_of_month\")\n",
    "    .groupby(\"first_day_of_month\")[\"microbusiness_density\"]\n",
    "    .mean()\n",
    "    + 0.55\n",
    ")\n",
    "\n",
    "t9.plot(ax=ax)\n",
    "\n",
    "plt.legend([\"Louisiana\", \"Mississippi\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe5902-58ad-434e-9dea-deecb2b3467d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_states = t5[abs(t5[\"corr\"]) > 0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009a128-59d4-482e-9c19-959392d3070d",
   "metadata": {},
   "source": [
    "#### State corr shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328f639-2b21-4965-968e-1ec329e749f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = (\n",
    "    df_train.groupby([\"state\", \"first_day_of_month\"])[\"microbusiness_density\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "state = \"Hawaii\"\n",
    "\n",
    "t7 = t0.pivot_table(\"mean\", \"first_day_of_month\", \"state\")\n",
    "a = t7[state].shift(-6).copy()\n",
    "a.index = t7.index\n",
    "t7[state] = a\n",
    "\n",
    "t1 = t7.sort_index().corr()\n",
    "\n",
    "plt.imshow(t1.values, cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7302d-aa09-424c-bc55-d628861a2e0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9177c66-9fd0-4587-b95e-b43024645f29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t5 = t1.rename_axis([\"other_state\"], axis=1).stack().rename(\"corr\").reset_index()\n",
    "t5 = t5[t5[\"state\"] != t5[\"other_state\"]]\n",
    "\n",
    "# Clean pairs of same correlations\n",
    "t5 = t5.sort_values(\"corr\").reset_index(drop=True)\n",
    "cols = [\"state\", \"other_state\"]\n",
    "t5[cols] = pd.DataFrame(np.sort(t5[cols].values, axis=1), columns=cols)\n",
    "t5 = t5.drop_duplicates()\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.01, min_samples=2).fit(t5[\"corr\"].values.reshape(-1, 1))\n",
    "\n",
    "t5[\"cluster\"] = clustering.labels_\n",
    "t5.groupby([\"cluster\"])[\"corr\"].agg([\"mean\", \"std\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e117bb-183f-46ba-898e-a56a5b201a3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t5[t5[\"state\"] == state].sort_values(\"corr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1a18e-18e2-461e-b176-bed39aaea377",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ax = df_train[df_train[\"state\"] == \"Alabama\"].sort_values(\"first_day_of_month\").groupby(\n",
    "#     \"first_day_of_month\"\n",
    "# )[\"microbusiness_density\"].mean().plot()\n",
    "\n",
    "k = df_train[df_train[\"state\"] == state].copy()\n",
    "k[\"first_day_of_month\"] = k[\"first_day_of_month\"]  # - pd.DateOffset(months=6)\n",
    "\n",
    "ax = (\n",
    "    k.sort_values(\"first_day_of_month\")\n",
    "    .groupby(\"first_day_of_month\")[\"microbusiness_density\"]\n",
    "    .mean()\n",
    "    .plot()\n",
    ")\n",
    "\n",
    "# ax = df_train[df_train[\"state\"] == \"Mississippi\"].sort_values(\"first_day_of_month\").groupby(\n",
    "#     \"first_day_of_month\"\n",
    "# )[\"microbusiness_density\"].mean().plot(ax=ax)\n",
    "\n",
    "other = \"Massachusetts\"\n",
    "t9 = (\n",
    "    df_train[df_train[\"state\"] == other]\n",
    "    .sort_values(\"first_day_of_month\")\n",
    "    .groupby(\"first_day_of_month\")[\"microbusiness_density\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "t9.plot(ax=ax)\n",
    "\n",
    "plt.legend([state, other])\n",
    "\n",
    "ax.grid(\"on\", which=\"minor\", axis=\"x\")\n",
    "ax.grid(\"off\", which=\"major\", axis=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bc765-c7e7-4e2e-aeb2-4198fabde3fe",
   "metadata": {},
   "source": [
    "#### Count up/down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13024f2d-a275-47db-9172-9c823360ed8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ups_downs(df_train):\n",
    "    col_to_group = \"cfips\"\n",
    "\n",
    "    t = df_train.copy()\n",
    "    t[\"microbusiness_shift_diff\"] = (\n",
    "        t[\"microbusiness_density\"]\n",
    "        - df_train.sort_values([col_to_group, \"first_day_of_month\"])\n",
    "        .groupby(col_to_group)\n",
    "        .shift()[\"microbusiness_density\"]\n",
    "    )\n",
    "\n",
    "    idx_over_0 = t[t[\"microbusiness_shift_diff\"] >= 0].index\n",
    "    idx_under_0 = t[t[\"microbusiness_shift_diff\"] < 0].index\n",
    "\n",
    "    t.loc[idx_over_0, \"microbusiness_shift_bool_over\"] = True\n",
    "    t.loc[idx_under_0, \"microbusiness_shift_bool_over\"] = False\n",
    "\n",
    "    t[\"microbusiness_shift_bool_over_sum\"] = (\n",
    "        t.groupby(col_to_group)[\"microbusiness_shift_bool_over\"]\n",
    "        .expanding()\n",
    "        .sum()\n",
    "        .values\n",
    "    )\n",
    "    t[\"microbusiness_shift_bool_over_count\"] = (\n",
    "        t.groupby(col_to_group)[\"microbusiness_shift_bool_over\"]\n",
    "        .expanding()\n",
    "        .count()\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    t[\"microbusiness_shift_bool_over_pct\"] = (\n",
    "        t[\"microbusiness_shift_bool_over_sum\"]\n",
    "        / t[\"microbusiness_shift_bool_over_count\"]\n",
    "    )\n",
    "\n",
    "    idx = t[t[\"microbusiness_shift_bool_over_count\"] < 3].index\n",
    "    t.loc[idx, \"microbusiness_shift_bool_over_pct\"] = np.nan\n",
    "\n",
    "    return t[[\"row_id\", \"microbusiness_shift_bool_over_pct\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f777182-7eee-4d76-87d2-03a735fb1a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ups_downs = ups_downs(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a487d1b-6b9b-4139-bb41-c3862ab5124e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ups_downs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a951d-6543-4d9a-bc0e-710d89a84615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9f93b-bd1b-4fa0-b676-2943e18f5b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.groupby(\"first_day_of_month\")[\"active\"].sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68feee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
