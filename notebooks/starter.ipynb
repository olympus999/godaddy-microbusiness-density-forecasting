{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8988304-69fc-4e3d-a15e-b7da71b87192",
   "metadata": {},
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada2126-7935-4780-b60b-62d39225d607",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import/read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590acc4-31dc-4c94-8155-19e68c214486",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from types import SimpleNamespace\n",
    "from typing import Callable\n",
    "import copy\n",
    "\n",
    "import bayes_opt as bayes\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bayes_opt import (\n",
    "    BayesianOptimization,\n",
    "    SequentialDomainReductionTransformer,\n",
    "    UtilityFunction,\n",
    ")\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.util import load_logs\n",
    "from library.classes import Feature, ManageDataSplit, ManageFeatures, feature_objects\n",
    "from library.classes.trait import DataFrame\n",
    "from library.feature_func import (\n",
    "    add_categorical_feature,\n",
    "    add_feature_targets_groupby_stats,\n",
    "    add_numerical_feature,\n",
    "    f_rolling_mean,\n",
    "    f_shifted,\n",
    "    time_arrow,\n",
    "    f_microbusiness_pct_change,\n",
    "    f_microbusiness_density_diff,\n",
    ")\n",
    "from library.optimize_this import optimize_this\n",
    "from library.utils import build_callbacks, read_df, smape, states, states_abb, write_df\n",
    "from meteostat import Monthly, Point, Stations\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from traitlets import (\n",
    "    Any,\n",
    "    Bool,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Float,\n",
    "    HasTraits,\n",
    "    Int,\n",
    "    List,\n",
    "    TraitError,\n",
    "    TraitType,\n",
    "    Tuple,\n",
    "    Unicode,\n",
    "    default,\n",
    "    validate,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7b749-b179-491b-84de-b586c94dbd71",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = \"../data/\"\n",
    "boundaries_sub_data_path = \"other/boundaries\"\n",
    "# bayesian_run_path = \"../data/bayesian_runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a255db8-0b36-465b-a5a0-90471a4beb29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_census = read_df(\"census_starter.csv\")\n",
    "df_test = read_df(\"test.csv\")\n",
    "df_train = read_df(\"train.csv\")\n",
    "df_submission = read_df(\"sample_submission.csv\")\n",
    "# df_population = read_df('df_population.csv', 'other')\n",
    "# df_census_population = read_df('df_census_population.csv', 'kaggle_census')\n",
    "# df_train_census = read_df('df_train_census.csv', 'kaggle_census')\n",
    "df_adjusted_microbusiness_density = read_df(\n",
    "    \"df_adjusted_microbusiness_density.csv\", \"kaggle_census\"\n",
    ")\n",
    "\n",
    "# df_boundaries = read_df(\"us-county-boundaries.csv\", boundaries_sub_data_path, delimiter=\";\")\n",
    "df_train = pd.merge(df_train, df_adjusted_microbusiness_density, \"left\", \"row_id\")\n",
    "# df_train = df_train.rename(\n",
    "#     columns={\n",
    "#         \"microbusiness_density\": \"original_microbusiness_density\",\n",
    "#         \"adjusted_microbusiness_density\": \"microbusiness_density\",\n",
    "#     }\n",
    "# )\n",
    "\n",
    "df_train = df_train.rename(\n",
    "    columns={\n",
    "        \"microbusiness_density\": \"original_microbusiness_density\",\n",
    "        \"active\": \"microbusiness_density\",\n",
    "    }\n",
    ")\n",
    "df_train[\"microbusiness_density\"] = df_train[\"microbusiness_density\"].apply(np.log1p)\n",
    "\n",
    "df_location = read_df(\"cfips_location.csv\", \"usa-counties-coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6aafbe-6bd4-4ab9-8c8e-e04af6fc517a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = df_train[df_train.isna().any(axis=1)]\n",
    "if t.shape[0] != 24:\n",
    "    raise Exception(\"Nan counts used to be 24... something changed\")\n",
    "df_train[df_train.isna().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26862e-21d2-48ef-9acb-48e8a6fdb8d2",
   "metadata": {},
   "source": [
    "### Weather save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b5b17-6d62-42d9-ab11-fb4cf193fcf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_weather_data(path_weather):\n",
    "    # Temperatures\n",
    "    temps = df_boundaries[[\"NAME\", \"NAMELSAD\", \"INTPTLAT\", \"INTPTLON\"]].copy()\n",
    "    temps[\"min_date\"] = df_train[\"first_day_of_month\"].min()\n",
    "    temps[\"max_date\"] = df_train[\"first_day_of_month\"].max()\n",
    "\n",
    "    data_list = []\n",
    "    for idx, row in temps.iterrows():\n",
    "        p = Point(row[\"INTPTLAT\"], row[\"INTPTLON\"], 70)\n",
    "\n",
    "        data = Monthly(p, row[\"min_date\"], row[\"max_date\"])\n",
    "        data = data.fetch()\n",
    "\n",
    "        if data.shape[0] > 0:\n",
    "            data[\"state\"] = row[\"NAME\"]\n",
    "            data[\"county\"] = row[\"NAMELSAD\"]\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "\n",
    "    weather_data = pd.concat(data_list)\n",
    "    weather_data.to_csv(path_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3185d92-86b2-43c8-807f-4fc0cd9bd87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_weather = \"../data/other/weather/weather.csv\"\n",
    "# save_weather_data(path_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134983df-a2b2-461b-bbb5-2809a2d7b622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv(path_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99894c95-7057-452e-93e8-22291f9552cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda2a9c-3c75-4fc7-8700-8884a9f83b40",
   "metadata": {},
   "source": [
    "### Feature setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e4b4e-744a-422f-a9f2-4b66bdd614f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "manage_data_split = ManageDataSplit(df_train)\n",
    "\n",
    "enabled_tuple = (0, 0.55)\n",
    "enabled_tuple_h = (0, 0.75)\n",
    "enabled_tuple_almost = (0.4, 1)\n",
    "enabled_tuple_always = (1, 1)\n",
    "params_tuple = (0, 10)\n",
    "\n",
    "gamma = 0\n",
    "\n",
    "# Better to clear it. Otherwise it might containt features we do not want\n",
    "feature_objects.clear()\n",
    "\n",
    "for cols in [\n",
    "    {\n",
    "        \"target_col\": \"microbusiness_density\",\n",
    "        \"groupby_col\": \"cfips\",\n",
    "        \"params_bounds\": list(repeat((1, 12), 4)),\n",
    "    },\n",
    "    # {\"target_col\": \"microbusiness_density\", \"groupby_col\": \"county\"},\n",
    "    # {\"target_col\": \"microbusiness_density\", \"groupby_col\": \"state\"},\n",
    "]:\n",
    "    target_col = cols[\"target_col\"]\n",
    "    groupby_col = cols[\"groupby_col\"]\n",
    "    params_bounds = list(repeat((1, 20), 4))\n",
    "    if \"params_bounds\" in cols.keys():\n",
    "        params_bounds = cols[\"params_bounds\"]\n",
    "    f_col = \"{}_{}_rolling_mean\".format(groupby_col, target_col)\n",
    "    feature_target_rolling_mean = Feature(\n",
    "        f_col,\n",
    "        f_rolling_mean,\n",
    "        df_train,\n",
    "        target_col=target_col,\n",
    "        groupby_col=groupby_col,\n",
    "        enabled_bounds=list(repeat(enabled_tuple, 4)),\n",
    "        params_bounds=params_bounds,\n",
    "    )\n",
    "\n",
    "\n",
    "target_col = \"microbusiness_density\"\n",
    "params_bounds = list(repeat((2, 10), 4))\n",
    "params_bounds[0] = (1, 1+gamma)\n",
    "enabled_bounds = list(repeat(enabled_tuple, 4))\n",
    "enabled_bounds[0] = enabled_tuple_always\n",
    "feature_target_shift_mean = Feature(\n",
    "    f_col=\"{}_shift\".format(target_col),\n",
    "    f=f_shifted,\n",
    "    df=df_train,\n",
    "    target_col=target_col,\n",
    "    enabled_bounds=enabled_bounds,\n",
    "    params_bounds=params_bounds,\n",
    ")\n",
    "\n",
    "target_col = \"original_microbusiness_density\"\n",
    "params_bounds = list(repeat((2, 10), 4))\n",
    "enabled_bounds = list(repeat(enabled_tuple, 4))\n",
    "params_bounds[0] = (1, 1+gamma)\n",
    "enabled_bounds[0] = enabled_tuple_always\n",
    "params_bounds[1] = (2, 2+gamma)\n",
    "enabled_bounds[1] = enabled_tuple_almost\n",
    "feature_target_shift_mean = Feature(\n",
    "    f_col=\"{}_shift\".format(target_col),\n",
    "    f=f_shifted,\n",
    "    df=df_train,\n",
    "    target_col=target_col,\n",
    "    enabled_bounds=enabled_bounds,\n",
    "    params_bounds=params_bounds,\n",
    ")\n",
    "\n",
    "for idx, col in enumerate(\n",
    "    [\n",
    "        (\"county\", enabled_tuple_almost),\n",
    "        \"state\",\n",
    "        (\"cfips\", enabled_tuple_almost),\n",
    "    ]\n",
    "):\n",
    "    _tuple = enabled_tuple\n",
    "    if type(col) is not str:\n",
    "        _tuple = col[1]\n",
    "        col = col[0]\n",
    "    _feature = Feature(col, add_categorical_feature, df_train, enabled_bounds=[_tuple])\n",
    "\n",
    "for idx, col in enumerate(\n",
    "    [\n",
    "        \"median_hh_inc\",\n",
    "        \"pct_bb\",\n",
    "        \"pct_college\",\n",
    "        \"pct_foreign_born\",\n",
    "        \"pct_it_workers\",\n",
    "        (\"target_census_over_18_population_x1000\", enabled_tuple_almost),\n",
    "        (\"target_census_population_x1000\", enabled_tuple_almost),\n",
    "        \"lng\",\n",
    "        \"lat\",\n",
    "        \"rot_15_x\",\n",
    "        \"rot_15_y\",\n",
    "        \"rot_30_x\",\n",
    "        \"rot_30_y\",\n",
    "        \"rot_45_x\",\n",
    "        \"rot_45_y\",\n",
    "    ]\n",
    "):\n",
    "    _tuple = enabled_tuple\n",
    "    if type(col) is not str:\n",
    "        _tuple = col[1]\n",
    "        col = col[0]\n",
    "    _feature = Feature(col, add_numerical_feature, df_train, enabled_bounds=[_tuple])\n",
    "\n",
    "feature_time_arrow = Feature(\n",
    "    \"time_arrow\", time_arrow, df_train, enabled_bounds=[enabled_tuple]\n",
    ")\n",
    "\n",
    "# for groupby_col in [\n",
    "#     \"cfips\",\n",
    "#     \"state\",\n",
    "#     \"county\",\n",
    "# ]:\n",
    "#     for col in [\n",
    "#         \"median_hh_inc\",\n",
    "#         \"pct_bb\",\n",
    "#         \"pct_college\",\n",
    "#         \"pct_foreign_born\",\n",
    "#         \"pct_it_workers\",\n",
    "#     ]:\n",
    "#         for agg_function in [\"median\", \"mean\", \"std\"]:\n",
    "#             f_col = \"{}_{}_target_{}\".format(groupby_col, col, agg_function)\n",
    "#             _feature = Feature(\n",
    "#                 f_col,\n",
    "#                 add_feature_targets_groupby_stats,\n",
    "#                 df_train,\n",
    "#                 groupby_col=groupby_col,\n",
    "#                 col=col,\n",
    "#                 agg_function=agg_function,\n",
    "#                 enabled_bounds=[enabled_tuple],\n",
    "#                 train_idx=manage_data_split._train_idx,\n",
    "#             )\n",
    "\n",
    "feature_microbusiness_density_pct_change = Feature(\n",
    "    \"microbusiness_density_pct_change\",\n",
    "    f_microbusiness_pct_change,\n",
    "    df_train,\n",
    "    enabled_bounds=[enabled_tuple],\n",
    ")\n",
    "\n",
    "feature_microbusiness_density_diff = Feature(\n",
    "    \"microbusiness_density_diff\",\n",
    "    f_microbusiness_density_diff,\n",
    "    df_train,\n",
    "    enabled_bounds=[enabled_tuple],\n",
    ")\n",
    "\n",
    "# Bounded region of parameter space\n",
    "model_pbounds = {\n",
    "    \"num_leaves\": (3, 200),\n",
    "    \"num_iterations\": (2000, 2000),\n",
    "    \"learning_rate\": (0.01, 2.5),\n",
    "    \"bagging_fraction\": (0.0001, 1),\n",
    "    \"feature_fraction\": (0.0001, 1),\n",
    "    \"lambda_l1\": (0, 500),\n",
    "    \"lambda_l2\": (0, 500),\n",
    "    \"bagging_freq\": (0, 500),\n",
    "    \"min_data_in_leaf\": (10, 3000),\n",
    "    \"min_sum_hessian_in_leaf\": (0, 500),\n",
    "    \"max_depth\": (-10, 150),\n",
    "    \"path_smooth\": (0, 500),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45bf33-522a-4b36-824a-7da6da47cf8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manage_data_split._train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc91f8-8436-4758-ac39-fd1786ebd4f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manage_data_split._val_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04970c5-0ba2-4bb0-9c80-d347889a5216",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manage_features = ManageFeatures(feature_objects)\n",
    "# manage_features.set_model_pbounds(model_pbounds)\n",
    "\n",
    "# pbounds = manage_features.get_pbounds()\n",
    "\n",
    "\n",
    "# feature = feature_target_shift_mean\n",
    "# df_mapped = manage_features._make_mapped(d)\n",
    "# df_mapped_feature = df_mapped[df_mapped[\"f_col\"] == feature.f_col]\n",
    "# r = feature.f(\n",
    "#     df=feature.df.copy(),\n",
    "#     df_mapped_feature=df_mapped_feature,\n",
    "#     f_col=feature.f_col,\n",
    "#     **feature._kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a60cb81-43a1-4ce6-abdf-9cf531b9e109",
   "metadata": {},
   "source": [
    "#### Start bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f689bb-3230-4c48-8057-36aaf32679c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "manage_features = ManageFeatures(feature_objects)\n",
    "manage_features.set_model_pbounds(model_pbounds)\n",
    "\n",
    "pbounds = manage_features.get_pbounds()\n",
    "\n",
    "pbounds = {**pbounds, 'lower_quantile': (0, 0.01), 'upper_quantile': (0.98, 1)}\n",
    "\n",
    "# acquisition_function = UtilityFunction(kind=\"ucb\")\n",
    "# acquisition_function = UtilityFunction(kind=\"poi\")\n",
    "# acquisition_function = UtilityFunction(kind=\"ucb\", kappa=0.1)\n",
    "# acquisition_function = UtilityFunction(kind=\"ucb\", kappa=1)\n",
    "# bounds_transformer = SequentialDomainReductionTransformer(minimum_window=0.5)\n",
    "\n",
    "objective = \"mae\"\n",
    "optimize_this_partial = partial(\n",
    "    optimize_this,\n",
    "    objective=objective,\n",
    "    pbounds=pbounds,\n",
    "    manage_data_split=manage_data_split,\n",
    "    manage_features=manage_features,\n",
    "    df_train=df_train,\n",
    "    build_callbacks=build_callbacks,\n",
    "    target_shift=0\n",
    ")\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_this_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=0,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=42,\n",
    "    # n_restarts_optimizer=50,\n",
    "    # bounds_transformer=bounds_transformer\n",
    ")\n",
    "\n",
    "# optimizer.set_gp_params(alpha=1e-2, n_restarts_optimizer=10)\n",
    "\n",
    "# load_logs(optimizer, logs=['../data/bayesian_optimizer/2023-03-05_14-19-15_logs.json'])\n",
    "\n",
    "# optimize_res = copy.deepcopy(optimizer.res)\n",
    "\n",
    "# df_optimizer_params = pd.DataFrame([x[\"params\"] for x in optimize_res])\n",
    "# df_optimizer_target = pd.DataFrame(\n",
    "#     [x[\"target\"] for x in optimize_res], columns=[\"target\"]\n",
    "# )\n",
    "\n",
    "# df_optimizer = pd.concat([df_optimizer_target, df_optimizer_params], axis=1)\n",
    "\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=optimize_this_partial,\n",
    "#     pbounds=pbounds,\n",
    "#     verbose=0,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "#     random_state=6,\n",
    "#     # bounds_transformer=bounds_transformer\n",
    "# )\n",
    "\n",
    "dt = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logger = JSONLogger(path=\"../data/bayesian_optimizer/{}_logs.json\".format(dt))\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# load_logs(optimizer, logs=[\"../data/bayesian_optimizer/2023-03-05_23-16-17_logs.json\"])\n",
    "\n",
    "# for idx, row in df_optimizer.sort_values(\"target\").tail(15).iterrows():\n",
    "#     optimizer.probe(\n",
    "#         params=optimize_res[idx][\"params\"]\n",
    "#     )\n",
    "\n",
    "# optimizer.set_gp_params(alpha=1, n_restarts_optimizer=10)\n",
    "optimizer.maximize(\n",
    "    init_points=20, n_iter=2000, \n",
    "    # acquisition_function=acquisition_function\n",
    ")\n",
    "\n",
    "print(optimizer.max[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97d52a-8f13-4c53-99fc-29d86c4c65a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "acquisition_function = UtilityFunction(kind=\"ucb\", kappa=0.5)\n",
    "optimizer.maximize(\n",
    "    init_points=0, n_iter=2000, \n",
    "    acquisition_function=acquisition_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bfb737-e995-4c54-8773-abc6170e97db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"../data/bayesian_optimizer/{}_logs.json\".format(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd0c85-26e6-46b9-90a2-850804b73ec4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_optimizer_params = pd.DataFrame([x['params'] for x in optimizer.res])\n",
    "df_optimizer_target = pd.DataFrame([x['target'] for x in optimizer.res], columns=['target'])\n",
    "\n",
    "df_optimizer = pd.concat([df_optimizer_target, df_optimizer_params], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a24f6-3ff3-46d4-b709-36f34f31a924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enabled_cols = [x for x in df_optimizer.columns if 'enabled_' in x]\n",
    "# df_optimizer[enabled_cols].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763a7de-b558-430a-bba6-5eec6710e5af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_optimizer.sort_values('target', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940f70f-64e3-471b-a89e-280c25547a51",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"../data/bayesian_optimizer/\"\n",
    "df_bay_logs = [\n",
    "    (x, os.path.getsize(os.path.join(path, x)))\n",
    "    for x in os.listdir(path)\n",
    "    if \"_logs.json\" in x\n",
    "]\n",
    "df_bay_logs = pd.DataFrame(df_bay_logs, columns=[\"filename\", \"size_mb\"])\n",
    "df_bay_logs[\"size_mb\"] = (df_bay_logs[\"size_mb\"] / 1024 / 1024).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb916f6a-814d-4226-b6eb-af774053f2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bay_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2bd72-0c57-4746-ad1e-26e2e9768946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764c97b-4036-48da-a6dc-2f2615bdc576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = optimizer.max['params']\n",
    "\n",
    "# keys_enabled = [x for x in params.keys() if 'enabled_' in x]\n",
    "# for k in keys_enabled:\n",
    "#     params[k] = 0\n",
    "    \n",
    "# keys_params = [x for x in params.keys() if 'params_' in x]\n",
    "# for k in keys_params:\n",
    "#     params[k] = 0\n",
    "\n",
    "# params['enabled_microbusiness_density_shift_0'] = 1\n",
    "# params['params_microbusiness_density_shift_0'] = 1\n",
    "\n",
    "# t = lgb.LGBMRegressor()\n",
    "# t_params = t.get_params()\n",
    "# for k in t_params.keys():\n",
    "#     if k in params.keys():\n",
    "#         print(k)\n",
    "#         params[k] = t_params[k]\n",
    "# # params = {**params, **t.get_params()}\n",
    "# params['bagging_fraction'] = 1\n",
    "# params['bagging_freq'] = 0\n",
    "# params['lambda_l1'] = 0\n",
    "# params['feature_fraction'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017259e3-0bcc-4435-a302-7b503db9e0ef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = optimizer.max['params']\n",
    "gbm, lgb_train, lgb_eval, lgb_test, model_params, callbacks, df_features, df_target = optimize_this_partial(\n",
    "    return_booster=True, **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b345e-9cd3-48e1-8cbe-c2613d88c06a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e84691-d8d6-44f6-b3cf-4f5c9eeec0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = lgb_test.data.copy()\n",
    "pred = gbm.predict(df_test)\n",
    "df_test['label'] = lgb_test.label\n",
    "df_test['pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a149830-e6e0-41c5-9a01-f3352f073221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2 = pd.merge(\n",
    "    df_test[[\"label\", \"pred\"]],\n",
    "    df_train.set_index(\"row_id\")[\n",
    "        [\"target_census_over_18_population_x1000\", \"original_microbusiness_density\"]\n",
    "    ],\n",
    "    \"left\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "t2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d23e1-fef4-4de8-a947-7c3c0d51d808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2['microbusiness_density_pred'] = (t2['pred'] / (t2['target_census_over_18_population_x1000'] * 1000)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed79c8-d663-4190-812e-8ff2a408f4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smape(t2['microbusiness_density_pred'], t2['original_microbusiness_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9323df4-836f-4a7e-ac5c-b61b88930c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(t2['label'] / (t2['target_census_population_x1000'] * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63e3f2-124e-4c6f-8678-93e150875692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104fde0-ad5a-41ed-b8ff-e824e7e2d6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2895f-76db-4381-9963-baa187222d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.set_index('row_id')['target_census_population_x1000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625451b-295d-4a19-bf97-0ee7d7b42f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88140e3f-c329-43f2-b2fb-3fcce9255748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(t.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4903597-7d34-44c4-9b52-b68b61a61aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8e2f2-1f17-4ca5-8c62-0568e297392e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = lgb_test\n",
    "pred = gbm.predict(dataset.data)\n",
    "smape(pred, dataset.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7feb41-323c-431e-ab0f-66d37544b668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t4 = df_optimizer.sort_values('target', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4324a-dab2-4314-9520-bfa31c97e58d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_features.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
